import Foundation
import SwiftUI

@MainActor
class AppState: ObservableObject, SpeechRecognitionDelegate {
    
    // MARK: - Navigation States (ìƒˆë¡œìš´ 3ë‹¨ê³„ ì‹œìŠ¤í…œ)
    enum NavigationStage: String, CaseIterable {
        case sttScanningMode = "STT_Scanning_Mode"           // Stage 1: STT + AR ìŠ¤ìº” (Live API ì—°ê²° ì—†ìŒ)
        case liveGuidanceMode = "Live_Guidance_Mode"         // Stage 2: Live API + ì£¼ê¸°ì  ê°€ì´ë˜ìŠ¤
        case pureConversationMode = "Pure_Conversation_Mode" // Stage 3: ìˆœìˆ˜ ëŒ€í™”
    }
    
    // MARK: - Published Properties
    @Published var currentStage: NavigationStage = .sttScanningMode
    @Published var requestedObjectNameByUser: String = ""
    @Published var confirmedDetrObjectName: String? = nil
    @Published var isNavigationActive: Bool = false
    @Published var lastDetectedSpeech: String = "" // For debugging STT
    
    // MARK: - Stage 1 Specific Properties
    @Published var scannedObjectLabels: Set<String> = [] // 360ë„ ìŠ¤ìº” ì¤‘ ëˆ„ì ëœ ê°ì²´ë“¤
    @Published var scanProgress: Float = 0.0 // 360ë„ íšŒì „ ì§„í–‰ë¥  (0.0 ~ 1.0)
    @Published var isFullRotationComplete: Bool = false
    @Published var scanCompleted: Bool = false // âœ… ì¶”ê°€: ìŠ¤ìº” ì™„ë£Œ í”Œë˜ê·¸ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
    @Published var objectMatchingInProgress: Bool = false // âœ… ì¶”ê°€: ê°ì²´ ë§¤ì¹­ ì§„í–‰ í”Œë˜ê·¸ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
    
    // **Stage 1â†’2 ì „í™˜ì„ ìœ„í•œ ì¤‘ì‹¬ë„ ê¸°ì¤€**
    private let requiredCenteredness: CGFloat = 0.85 // âœ… 85% ì¤‘ì‹¬ë„ + 1ì´ˆ ìœ ì§€ í•„ìš”
    
    // MARK: - Stage 2 Specific Properties  
    @Published var guidanceTimer: Timer? = nil // 3ì´ˆë§ˆë‹¤ ìë™ í”„ë¡¬í”„íŠ¸ìš© íƒ€ì´ë¨¸
    @Published var lastGuidanceTime: Date = Date.distantPast
    @Published var guidanceRequestCount: Int = 0 // ê°€ì´ë˜ìŠ¤ ìš”ì²­ íšŸìˆ˜
    
    // MARK: - Audio Integration
    @Published var audioManager: AudioManager? = nil
    
    // MARK: - STT Integration
    @Published var speechManager: SpeechRecognitionManager
    
    // MARK: - AR Integration
    weak var arViewModel: ARViewModel?
    
    // MARK: - Gemini Integration  
    weak var geminiClient: GeminiLiveAPIClient?
    
    init() {
        self.speechManager = SpeechRecognitionManager()
        // AudioManagerëŠ” ë‚˜ì¤‘ì— setupAudioManager()ì—ì„œ ì´ˆê¸°í™”
        Task { @MainActor in
            self.speechManager.delegate = self
            // AudioManager ì´ˆê¸°í™”ëŠ” Gemini Live API ì—°ê²° í›„ë¡œ ì—°ê¸°
        }
    }
    
    // âœ… AudioManager ëŠ¦ì€ ì´ˆê¸°í™” ë©”ì„œë“œ
    private func setupAudioManager() {
        guard audioManager == nil else { return }
        
        print("AppState: Setting up AudioManager after Gemini Live API initialization")
        audioManager = AudioManager()
        audioManager?.checkAvailableAudioFiles()
        print("AppState: âœ… AudioManager setup completed")
    }
    
    // MARK: - STT Control
    func startListeningForKeywords() {
        guard currentStage == .sttScanningMode else {
            print("AppState: Not starting STT - not in STT scanning stage")
            return
        }
        
        // **Stage 1ì—ì„œëŠ” Gemini ì†Œì¼“ ì—°ê²°í•˜ì§€ ì•ŠìŒ**
        if let geminiClient = geminiClient, geminiClient.isConnected {
            print("AppState: Disconnecting Gemini socket for Stage 1 (STT only)")
            geminiClient.disconnect()
        }
        
        // STT ì‹œì‘
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.speechManager.startListening()
            print("AppState: Started listening for Korean keywords in Stage 1")
        }
    }
    
    func stopListeningForKeywords() {
        speechManager.stopListening()
        print("AppState: Stopped listening for keywords")
    }
    
    // MARK: - SpeechRecognitionDelegate
    nonisolated func didDetectFindRequest(for objectName: String, in fullText: String) {
        Task { @MainActor in
            // Stage 1ì—ì„œë§Œ ê°ì²´ ì°¾ê¸° ìš”ì²­ ì²˜ë¦¬
            guard currentStage == .sttScanningMode else {
                print("AppState: Ignoring find request - not in STT scanning stage")
                return
            }
            
            lastDetectedSpeech = fullText
            setRequestedObject(objectName)
            
            // STT ì¤‘ì§€
            speechManager.stopListening()
            print("AppState: STT stopped after keyword detection in Stage 1")
            
            // âœ… 10ì´ˆ ìŠ¤ìº”ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´ ì¦‰ì‹œ ê°ì²´ ë§¤ì¹­ ì§„í–‰
            if scanCompleted && !scannedObjectLabels.isEmpty {
                print("AppState: 10-second scan already completed, proceeding with object matching")
                requestGeminiObjectMatching()
            } else {
                print("AppState: 10-second scan not completed yet, starting AR scanning")
                // AR ìŠ¤ìº” ì‹œì‘
                startARScanning()
            }
        }
    }
    
    // MARK: - Stage Management
    func transitionTo(_ newStage: NavigationStage) {
        let previousStage = currentStage
        currentStage = newStage
        
        print("AppState: Transitioned from \(previousStage.rawValue) to \(newStage.rawValue)")
        
        isNavigationActive = (newStage == .liveGuidanceMode || newStage == .pureConversationMode)
        
        handleStageEntry(newStage, from: previousStage)
    }
    
    private func handleStageEntry(_ stage: NavigationStage, from previousStage: NavigationStage) {
        switch stage {
        case .sttScanningMode:
            print("AppState: === STAGE 1: STT + AR Scanning Mode ===")
            
            // Reset all data
            requestedObjectNameByUser = ""
            confirmedDetrObjectName = nil
            isNavigationActive = false
            lastDetectedSpeech = ""
            scannedObjectLabels = []
            scanProgress = 0.0
            isFullRotationComplete = false
            scanCompleted = false // âœ… ì¶”ê°€: ìŠ¤ìº” ì™„ë£Œ í”Œë˜ê·¸ ë¦¬ì…‹
            objectMatchingInProgress = false // âœ… ì¶”ê°€: ê°ì²´ ë§¤ì¹­ í”Œë˜ê·¸ ë¦¬ì…‹
            guidanceRequestCount = 0
            
            stopPeriodicGuidance()
            
            // Disconnect Gemini for Stage 1 (STT only)
            if let geminiClient = geminiClient, geminiClient.isConnected {
                print("AppState: Disconnecting Gemini for Stage 1 (STT only)")
                geminiClient.disconnect()
            }
            
            arViewModel?.stopScanning()
            arViewModel?.stopHapticGuidance()
            
            // âœ… Audio-driven workflow: í™˜ì˜ ë©”ì‹œì§€ í›„ STT ì‹œì‘
            if audioManager == nil {
                print("AppState: AudioManager not ready, setting up temporarily for Stage 1")
                setupAudioManager()
            }
            
            // âœ… ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: 10ì´ˆ ìŠ¤ìº” â†’ ìë™ ì§ˆë¬¸
            audioManager?.playWelcomeRotateAudio { [weak self] in
                print("AppState: Welcome audio completed, starting 10-second scan")
                self?.startTenSecondScan()
            }
            
        case .liveGuidanceMode:
            print("AppState: === STAGE 2: Live Guidance Mode ===")
            
            speechManager.stopListening()
            print("AppState: STT stopped before Gemini connection")
            
            // âœ… KEEP HAPTIC: í–…í‹± ê°€ì´ë˜ìŠ¤ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
            // arViewModel?.stopHapticGuidance()  // âŒ ì£¼ì„ ì²˜ë¦¬
            
            // âœ… Audio confirmation: íƒ€ê²Ÿ ë½ì˜¨ ì•ˆë‚´
            if let distance = arViewModel?.distanceToObject {
                audioManager?.playTargetLockedAudio(distance: distance) { [weak self] in
                    print("AppState: Target locked audio completed, starting guidance")
                    self?.connectToGeminiLiveAPI()
                    DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                        self?.startPeriodicGuidance()
                    }
                }
            } else {
                // ê±°ë¦¬ ì •ë³´ ì—†ì„ ê²½ìš°ì—ë„ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©
                audioManager?.playTargetLockedAudio(distance: 0.0) { [weak self] in
                    print("AppState: Target locked audio completed, starting guidance")
                    self?.connectToGeminiLiveAPI()
                    DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
                        self?.startPeriodicGuidance()
                    }
                }
            }
            
        case .pureConversationMode:
            print("AppState: === STAGE 3: Pure Conversation Mode ===")
            
            stopPeriodicGuidance()
            
            // âœ… STAGE 3: ë¬´ì¡°ê±´ í–…í‹± í”¼ë“œë°± ì¤‘ë‹¨ (Pure Conversation Mode)
            arViewModel?.stopHapticGuidance()
            print("AppState: Stopped haptic guidance - entering pure conversation mode")
            
            // âœ… ì¶”ê°€: íƒ€ê²Ÿ ì¶”ì  ì™„ì „ ì¤‘ë‹¨
            arViewModel?.userTargetObjectName = ""
            print("AppState: Cleared target object name - no more tracking")
            
            // âœ… ê°•í™”: Gemini Live API ì˜¤ë””ì˜¤ ì™„ì „ ì¤‘ë‹¨ (ì¶œë ¥ë§Œ, ì…ë ¥ì€ ìœ ì§€)
            if let geminiClient = geminiClient {
                // í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ì˜¤ë””ì˜¤ ì¦‰ì‹œ ì¤‘ë‹¨
                geminiClient.stopCurrentAudioPlayback()
                // âœ… ìˆ˜ì •: ë…¹ìŒì€ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ (ìœ ì € ì…ë ¥ ìœ ì§€)
                // geminiClient.stopRecording() // âŒ ì£¼ì„ ì²˜ë¦¬
                // AI ìƒíƒœ í”Œë˜ê·¸ ë¦¬ì…‹
                geminiClient.resetAISpeakingState()
                
                // âœ… ì¶”ê°€: 3ë‹¨ê³„ì—ì„œ ë…¹ìŒì´ í™œì„±í™”ë˜ë„ë¡ ë³´ì¥
                if geminiClient.isConnected && !geminiClient.isRecording {
                    print("AppState: Stage 3 - Ensuring recording is active for user input")
                    geminiClient.startRecording()
                }
                
                print("AppState: âœ… Stopped Gemini audio output but kept recording for user input")
            }
            
            // âœ… AudioManagerì˜ í˜„ì¬ ì¬ìƒë„ ì¤‘ë‹¨
            audioManager?.stopAudio()
            
            // ê±°ë¦¬ì— ë”°ë¥¸ ë‹¤ë¥¸ ì˜¤ë””ì˜¤ ì•ˆë‚´
            if let arViewModel = arViewModel, let distance = arViewModel.distanceToObject, distance < 0.3 {
                // âœ… Audio confirmation: ë„ì°© ì•ˆë‚´ (ê°€ê¹Œì´ ìˆì„ ë•Œ)
                audioManager?.playTargetReachedAudio(distance: distance) {
                    print("AppState: Target reached audio completed - pure conversation mode")
                }
            } else {
                // âœ… Audio: ì¼ë°˜ ì „í™˜ ì•ˆë‚´ (ë©€ë¦¬ ìˆì„ ë•Œë„ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©)
                audioManager?.playTargetReachedAudio(distance: 0.0) {
                    print("AppState: Target reached audio completed - pure conversation mode")
                }
            }
        }
    }
    
    // MARK: - Stage 1: AR Scanning Control
    
    // âœ… ìƒˆë¡œìš´ ë©”ì„œë“œ: 10ì´ˆê°„ ìë™ ìŠ¤ìº”
    private func startTenSecondScan() {
        guard currentStage == .sttScanningMode else {
            print("AppState: Not starting scan - not in STT scanning stage")
            return
        }
        
        print("AppState: ğŸ” Starting 10-second automatic scanning")
        print("AppState: â° Timer set for 10 seconds - will auto-ask for object")
        
        // AR ìŠ¤ìº” ì‹œì‘ (ê°ì²´ ì´ë¦„ ì—†ì´)
        arViewModel?.startScanning(for: "")
        
        // 10ì´ˆ ìŠ¤ìº” ì§„í–‰ ëª¨ë‹ˆí„°ë§
        monitorTenSecondScan()
        
        // 10ì´ˆ í›„ ìë™ìœ¼ë¡œ ì§ˆë¬¸ ì‹œì‘
        DispatchQueue.main.asyncAfter(deadline: .now() + 5.0) {
            self.completeTenSecondScanAndAskUser()
        }
    }
    
    private func monitorTenSecondScan() {
        guard currentStage == .sttScanningMode, !scanCompleted else { 
            return 
        }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            guard self.currentStage == .sttScanningMode, !self.scanCompleted,
                  let arViewModel = self.arViewModel else { 
                return 
            }
            
            // ìŠ¤ìº”ëœ ê°ì²´ë“¤ ëˆ„ì 
            self.scannedObjectLabels = arViewModel.allDetectedObjects
            
            // ê³„ì† ëª¨ë‹ˆí„°ë§ (10ì´ˆ ë™ì•ˆ)
            if !self.scanCompleted {
                self.monitorTenSecondScan()
            }
        }
    }
    
    private func completeTenSecondScanAndAskUser() {
        guard currentStage == .sttScanningMode, !scanCompleted else {
            print("AppState: âš ï¸ 10-second scan already completed or stage changed")
            return
        }
        
        scanCompleted = true // âœ… ìŠ¤ìº” ì™„ë£Œ í‘œì‹œ
        print("AppState: âœ… 10-second scan completed. Found \(scannedObjectLabels.count) objects: \(Array(scannedObjectLabels))")
        
        // AR ìŠ¤ìº” ì¤‘ì§€
        arViewModel?.stopScanning()
        
        // âœ… ì´ë¯¸ ìœ ì €ê°€ ê°ì²´ë¥¼ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
        if !requestedObjectNameByUser.isEmpty {
            print("AppState: User already requested object '\(requestedObjectNameByUser)', proceeding with matching")
            requestGeminiObjectMatching()
        } else {
            // ìë™ìœ¼ë¡œ ì§ˆë¬¸ ì˜¤ë””ì˜¤ ì¬ìƒ í›„ STT ì‹œì‘
            audioManager?.playAskObjectAudio { [weak self] in
                print("AppState: Ask what object audio completed, starting STT")
                self?.startListeningForKeywords()
            }
        }
    }
    
    private func startARScanning() {
        print("AppState: Starting AR scanning for: '\(requestedObjectNameByUser)'")
        
        // AR ìŠ¤ìº” ì‹œì‘
        arViewModel?.startScanning(for: requestedObjectNameByUser)
        
        // ìŠ¤ìº” ì§„í–‰ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitorScanProgress()
    }
    
    private func monitorScanProgress() {
        guard currentStage == .sttScanningMode, !scanCompleted else { 
            if scanCompleted {
                print("AppState: âš ï¸ Scan already completed, skipping monitorScanProgress")
            }
            return 
        }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            guard self.currentStage == .sttScanningMode, !self.scanCompleted,
                  let arViewModel = self.arViewModel else { 
                if self.scanCompleted {
                    print("AppState: âš ï¸ Scan completed during monitoring, stopping")
                }
                return 
            }
            
            // Sync detected objects from ARViewModel
            self.scannedObjectLabels = arViewModel.allDetectedObjects
            self.scanProgress = arViewModel.scanProgress
            
            if arViewModel.scanProgress >= 1.0 {
                print("AppState: ğŸ¯ Scan progress completed, calling completeScan()")
                self.completeScan()
            } else {
                self.monitorScanProgress()
            }
        }
    }
    
    private func completeScan() {
        // âœ… ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        guard !scanCompleted else {
            print("AppState: âš ï¸ completeScan() already called, ignoring duplicate call")
            return
        }
        
        guard let arViewModel = arViewModel else { return }
        
        scanCompleted = true // âœ… í”Œë˜ê·¸ ì„¤ì •ìœ¼ë¡œ ì¶”ê°€ í˜¸ì¶œ ë°©ì§€
        print("AppState: âœ… Scan completed. Found target: \(arViewModel.foundTarget)")
        
        // âœ… 10ì´ˆ ìë™ ìŠ¤ìº” ì¤‘ì´ì—ˆë‹¤ë©´ ë‹¨ìˆœíˆ ëˆ„ì ë§Œ í•˜ê³  completeTenSecondScanAndAskUserê°€ ì²˜ë¦¬
        if requestedObjectNameByUser.isEmpty {
            print("AppState: This was automatic 10-second scan, letting completeTenSecondScanAndAskUser handle it")
            // ê°ì²´ ëˆ„ì ë§Œ í•˜ê³  ë³„ë„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
            scannedObjectLabels = arViewModel.allDetectedObjects
            return
        }
        
        // âœ… ì‚¬ìš©ìê°€ íŠ¹ì • ê°ì²´ë¥¼ ìš”ì²­í•œ ìƒíƒœì—ì„œì˜ ìŠ¤ìº” ì™„ë£Œ
        if arViewModel.foundTarget {
            confirmedDetrObjectName = requestedObjectNameByUser
            arViewModel.userTargetObjectName = requestedObjectNameByUser
            startHapticGuidanceAndTransition()
        } else {
            requestGeminiObjectMatching()
        }
    }
    
    private func requestGeminiObjectMatching() {
        // âœ… ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        guard !objectMatchingInProgress else {
            print("AppState: âš ï¸ Object matching already in progress, ignoring duplicate call")
            return
        }
        
        guard let geminiClient = geminiClient,
              let arViewModel = arViewModel else {
            print("AppState: Missing geminiClient or arViewModel for object matching")
            
            // âœ… Audio-driven transition: ê°ì²´ ì—†ìŒ ì•ˆë‚´
            audioManager?.playObjectNotFoundAudio { [weak self] in
                print("AppState: Object not found audio completed, transitioning to Stage 3")
                self?.transitionTo(.pureConversationMode)
            }
            return
        }
        
        let detectedObjects = Array(scannedObjectLabels)
        guard !detectedObjects.isEmpty else {
            print("AppState: No objects detected for matching")
            
            // âœ… Audio-driven transition: ê°ì²´ ì—†ìŒ ì•ˆë‚´
            audioManager?.playObjectNotFoundAudio { [weak self] in
                print("AppState: Object not found audio completed, transitioning to Stage 3")
                self?.transitionTo(.pureConversationMode)
            }
            return
        }
        
        objectMatchingInProgress = true // âœ… í”Œë˜ê·¸ ì„¤ì •ìœ¼ë¡œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        print("AppState: ğŸ” Requesting Gemini API matching for '\(requestedObjectNameByUser)' among: \(detectedObjects)")
        
        // Connect temporarily for REST API matching
        if !geminiClient.isConnected {
            geminiClient.connect()
        }
        
        geminiClient.findSimilarObject(koreanObjectName: requestedObjectNameByUser, availableObjects: detectedObjects) { [weak self] matchedObject in
            guard let self = self else { return }
            
            // âœ… í”Œë˜ê·¸ í•´ì œ
            DispatchQueue.main.async {
                self.objectMatchingInProgress = false
            }
            
            if let matchedObject = matchedObject {
                print("AppState: âœ… Gemini matched '\(self.requestedObjectNameByUser)' to '\(matchedObject)'")
                self.confirmedDetrObjectName = matchedObject
                
                DispatchQueue.main.async {
                    self.arViewModel?.userTargetObjectName = matchedObject
                    
                    // âœ… Audio + Haptic: ë™ì‹œ ì‹œì‘ (Critical Requirement)
                    self.audioManager?.playObjectFoundHapticGuideAudio { [weak self] in
                        print("AppState: Object found haptic guide audio completed")
                    }
                    
                    // í–…í‹± ê°€ì´ë˜ìŠ¤ë„ ë™ì‹œì— ì‹œì‘
                    self.startHapticGuidanceAndTransition()
                }
            } else {
                print("AppState: âŒ Gemini could not find a match for '\(self.requestedObjectNameByUser)'")
                
                // âœ… Audio-driven transition: ê°ì²´ ì—†ìŒ ì•ˆë‚´
                self.audioManager?.playObjectNotFoundAudio { [weak self] in
                    print("AppState: Object not found audio completed, transitioning to Stage 3")
                    self?.transitionTo(.pureConversationMode)
                }
            }
        }
    }
    
    private func startHapticGuidanceAndTransition() {
        guard let confirmedObject = confirmedDetrObjectName else { return }
        
        print("AppState: Starting haptic guidance for: '\(confirmedObject)'")
        
        // í–…í‹± ê°€ì´ë“œ ì‹œì‘
        arViewModel?.startHapticGuidance(for: confirmedObject)
        
        // íƒ€ê²Ÿ ë„ë‹¬ ëª¨ë‹ˆí„°ë§
        monitorTargetReached()
    }
    
    private func monitorTargetReached() {
        guard currentStage == .sttScanningMode else { return }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            guard self.currentStage == .sttScanningMode,
                  let arViewModel = self.arViewModel else { return }
            
            // âœ… ARViewModelì˜ ì¤‘ì•™ íƒì§€ ì™„ë£Œ ìƒíƒœ í™•ì¸ (85%, 1ì´ˆ ìœ ì§€)
            let currentCenteredness = arViewModel.detectedObjectCenteredness
            let isCenterDetectionCompleted = arViewModel.isCenterDetectionActive
            
            if isCenterDetectionCompleted {
                // âœ… 85% ì¤‘ì‹¬ë„ë¥¼ 1ì´ˆê°„ ìœ ì§€ ì™„ë£Œ - Stage 2ë¡œ ì „í™˜
                print("AppState: Center detection completed (85%, 1 second) - Centeredness: \(String(format: "%.1f", currentCenteredness * 100))%. Transitioning to Stage 2")
                
                // âœ… ì¶”ê°€: ê±°ë¦¬ ì •ë³´ë„ ë¡œê¹…
                if let distance = arViewModel.distanceToObject {
                    print("AppState: Target distance: \(String(format: "%.2f", distance))m")
                }
                
                self.transitionTo(.liveGuidanceMode)
                return
            } else {
                // âœ… ì¤‘ì•™ íƒì§€ ì§„í–‰ ìƒíƒœ í”¼ë“œë°±
                let progress = arViewModel.centerDetectionProgress
                if progress > 0 {
                    print("AppState: Center detection in progress - \(String(format: "%.1f", currentCenteredness * 100))%, Progress: \(String(format: "%.0f", progress * 100))%")
                } else {
                    print("AppState: Centeredness insufficient (\(String(format: "%.1f", currentCenteredness * 100))%), need 85% for 1 second")
                }
            }
            
            // ê³„ì† ëª¨ë‹ˆí„°ë§
            self.monitorTargetReached()
        }
    }
    
    // MARK: - Stage 2: Periodic Guidance Control
    private func connectToGeminiLiveAPI() {
        guard let geminiClient = geminiClient else { return }
        
        // **ë‹¨ìˆœí™”: ì—°ê²°ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì—°ê²°**
        if !geminiClient.isConnected {
            print("AppState: Connecting to Gemini Live API for Stage 2")
            geminiClient.connect()
        } else {
            print("AppState: Gemini Live API already connected")
        }
    }
    
    private func startPeriodicGuidance() {
        stopPeriodicGuidance() // ê¸°ì¡´ íƒ€ì´ë¨¸ ì •ë¦¬
        
        print("AppState: Starting robust guidance system with 2-second intervals")
        
        // âœ… ì¦‰ì‹œ ì²« ê°€ì´ë˜ìŠ¤ ìš”ì²­ ì „ì†¡
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.sendPeriodicGuidanceRequest()
        }
        
        // âœ… ê²¬ê³ í•œ 2ì´ˆ ê°„ê²© íƒ€ì´ë¨¸ ì‹œìŠ¤í…œ
        guidanceTimer = Timer.scheduledTimer(withTimeInterval: 2.0, repeats: true) { [weak self] _ in
            guard let self = self, self.currentStage == .liveGuidanceMode else {
                print("AppState: Guidance timer stopped - stage changed")
                return
            }
            
            print("ğŸ”„ AppState: Timer tick - attempting guidance request")
            self.sendPeriodicGuidanceRequest()
        }
        
        print("AppState: âœ… Guidance timer started with 2s intervals")
        
        // **ì¶”ê°€: Stage 2ì—ì„œ Stage 3ë¡œì˜ ì „í™˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘**
        monitorDistanceForStage3Transition()
    }
    
    // **ì¶”ê°€: Stage 2 â†’ Stage 3 ì „í™˜ ëª¨ë‹ˆí„°ë§**
    private func monitorDistanceForStage3Transition() {
        guard currentStage == .liveGuidanceMode else { return }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            guard self.currentStage == .liveGuidanceMode,
                  let arViewModel = self.arViewModel else { return }
            
            // **Stage 3 ì „í™˜ ì¡°ê±´: 90% ì¤‘ì‹¬ë„ + 50cm ì´ë‚´ ì ‘ê·¼**
            let centeredness = arViewModel.detectedObjectCenteredness
            let distance = arViewModel.distanceToObject
            
            let isCenterConditionMet = centeredness > 0.8
            let isDistanceConditionMet = distance != nil && distance! < 0.7
            
            if isCenterConditionMet && isDistanceConditionMet {
                print("AppState: Stage 3 conditions met - Centeredness: \(String(format: "%.1f", centeredness * 100))%, Distance: \(String(format: "%.2f", distance!))m")
                self.transitionTo(.pureConversationMode)
            } else {
                // ì¡°ê±´ ë¯¸ì¶©ì¡± ì‹œ ë””ë²„ê·¸ ì •ë³´
                if !isCenterConditionMet {
                    print("AppState: Stage 3 - Centeredness insufficient: \(String(format: "%.1f", centeredness * 100))% (need 90%)")
                }
                if !isDistanceConditionMet {
                    if let d = distance {
                        print("AppState: Stage 3 - Distance too far: \(String(format: "%.2f", d))m (need <0.5m)")
                    } else {
                        print("AppState: Stage 3 - Distance not available")
                    }
                }
                
                // ê³„ì† ëª¨ë‹ˆí„°ë§
                self.monitorDistanceForStage3Transition()
            }
        }
    }
    
    private func stopPeriodicGuidance() {
        if guidanceTimer != nil {
            guidanceTimer?.invalidate()
            guidanceTimer = nil
            print("AppState: âœ… Periodic guidance timer stopped and invalidated")
        } else {
            print("AppState: âš ï¸ No guidance timer to stop (already nil)")
        }
        
        // âœ… ì¶”ê°€ ì •ë³´ ë¡œê·¸
        print("AppState: Current stage: \(currentStage)")
        print("AppState: Guidance request count: \(guidanceRequestCount)")
        
        // âœ… ìŠ¤ë§ˆíŠ¸ ê°€ì´ë˜ìŠ¤ ì‹œìŠ¤í…œë„ ì¤‘ë‹¨ (currentStage ë³€ê²½ìœ¼ë¡œ ìë™ ì¤‘ë‹¨ë¨)
        print("AppState: Smart guidance system will auto-stop on stage change")
    }
    
    private func sendPeriodicGuidanceRequest() {
        guard currentStage == .liveGuidanceMode,
              let geminiClient = geminiClient,
              let targetObject = confirmedDetrObjectName,
              let arViewModel = arViewModel else {
            print("âŒ AppState: Cannot send guidance request - missing requirements")
            print("   Stage: \(currentStage)")
            print("   GeminiClient: \(geminiClient != nil ? "âœ…" : "âŒ")")
            print("   Target: \(confirmedDetrObjectName ?? "nil")")
            print("   ARViewModel: \(arViewModel != nil ? "âœ…" : "âŒ")")
            return
        }
        
        // âœ… AIê°€ ë§í•˜ê³  ìˆìœ¼ë©´ ì´ë²ˆ ìš”ì²­ ìŠ¤í‚µ (ë‹¤ìŒ íƒ€ì´ë¨¸ì—ì„œ ì¬ì‹œë„)
        if !geminiClient.canSendGuidanceRequest() {
            print("â¸ï¸ AppState: Skipping guidance request - AI busy (speaking: \(geminiClient.isAISpeaking), pending: \(geminiClient.hasPendingGuidanceRequest))")
            return
        }
        
        let now = Date()
        let timeSinceLastGuidance = now.timeIntervalSince(lastGuidanceTime)
        
        // **ìˆ˜ì •: í”„ë¡¬í”„íŠ¸ì— ë³€í™” ìš”ì†Œ ì¶”ê°€ë¡œ ì‘ë‹µ ë‹¤ì–‘ì„± í™•ë³´**
        guidanceRequestCount += 1
        let timeString = DateFormatter.localizedString(from: now, dateStyle: .none, timeStyle: .medium)
        
        // âœ… ê±°ë¦¬ ì •ë³´ ì¶”ê°€
        let distanceInfo: String
        if let distance = arViewModel.distanceToObject {
            distanceInfo = "**í˜„ì¬ ì¸¡ì •ëœ ê±°ë¦¬: \(String(format: "%.1f", distance))ë¯¸í„°**"
        } else {
            distanceInfo = "**ê±°ë¦¬ ì¸¡ì • ì¤‘...**"
        }
        
        let prompt = """
        [Guidance Request #\(guidanceRequestCount) - \(timeString)]
        
        SYSTEM: You are providing periodic navigation guidance to a visually impaired user seeking "\(targetObject)".
        \(distanceInfo)
        
        ANALYZE THE CURRENT VIDEO FRAME RIGHT NOW - don't assume user has moved:
        
        1. IMMEDIATE SAFETY ALERT: Any moving hazards (people, vehicles, animals) or dangerous surfaces (stairs, holes, water)? 
           - If yes, start with "ğŸš¨ ì£¼ì˜:" and specify exact type, location, and immediate action needed
        
        2. OBSTACLE INVENTORY: List ALL visible obstacles between user and target:
           - HIGH obstacles (head/body level): chairs, tables, poles, trees, people, vehicles
           - LOW obstacles (foot level): steps, curbs, toys, bags, uneven surfaces
           - OVERHEAD obstacles: low branches, signs, awnings
           - For EACH obstacle, specify: type, location (left/right/center), estimated height, and width
        
        3. OBSTACLE AVOIDANCE STRATEGY: For each significant obstacle, provide specific bypass instructions:
           - "Xë¯¸í„° ì™¼ìª½ìœ¼ë¡œ ìš°íšŒí•´ì„œ ì§€ë‚˜ê°€ì„¸ìš”" 
           - "ë‚®ì€ ì¥ì• ë¬¼ì´ë‹ˆ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°€ì„¸ìš”"
           - "ë†’ì€ ì¥ì• ë¬¼ì´ë‹ˆ ì˜¤ë¥¸ìª½ìœ¼ë¡œ 2ê±¸ìŒ ëŒì•„ê°€ì„¸ìš”"
           - "ì›€ì§ì´ëŠ” ì¥ì• ë¬¼ì´ë‹ˆ ì ì‹œ ë©ˆì¶”ê³  ì§€ë‚˜ê°€ê¸¸ ê¸°ë‹¤ë¦¬ì„¸ìš”"
        
        4. PATH GUIDANCE: Recommend the safest clear path:
           - "ê°€ì¥ ì•ˆì „í•œ ê²½ë¡œ: í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì™¼ìª½ìœ¼ë¡œ 3ê±¸ìŒ, ê·¸ë‹¤ìŒ ì§ì§„ 5ê±¸ìŒ"
           - Include surface conditions: "í‰í‰í•œ ë°”ë‹¥" vs "ìš¸í‰ë¶ˆí‰í•œ ë°”ë‹¥" vs "ê³„ë‹¨"
        
        5. TARGET STATUS & NAVIGATION:
           - Current target visibility and exact location
           - Estimated distance and direction to target
           - Next immediate action with specific step count and direction
        
        6. ACCESSIBILITY FEATURES: Mention any helpful landmarks:
           - Tactile markers, handrails, walls to follow
           - Audio cues (traffic sounds, voices, machinery)
           - Distinctive textures or surfaces for orientation
        
        RESPONSE FORMAT: 
        - Start with safety alerts if any
        - Then obstacles and avoidance strategies  
        - Then path guidance
        - End with next action
        
        RESPOND IN KOREAN. Base everything on CURRENT video frame only. ë¹ ë¥¸ ì†ë„ë¡œ ë§í•´ì£¼ì„¸ìš”.
        
        Example: "ğŸš¨ ì£¼ì˜: ì•ìª½ 2ë¯¸í„°ì— ì˜ìê°€ ìˆì–´ìš”. ì˜¤ë¥¸ìª½ìœ¼ë¡œ 1ë¯¸í„° ìš°íšŒí•˜ì„¸ìš”. ê·¸ ë‹¤ìŒ ì§ì§„ 3ê±¸ìŒ ê°€ë©´ í‰í‰í•œ ë°”ë‹¥ì´ê³ , ëª©í‘œì¸ í…Œì´ë¸”ì´ ì •ë©´ì— ë³´ì—¬ìš”. ì§€ê¸ˆ ì˜¤ë¥¸ìª½ìœ¼ë¡œ 3ê±¸ìŒ ì´ë™í•˜ì„¸ìš”."
        """
        
        lastGuidanceTime = now
        
        // âœ… ê°„ë‹¨í•œ pending ìƒíƒœ ê´€ë¦¬
        geminiClient.hasPendingGuidanceRequest = true
        
        print("ğŸ”„ AppState: Sending guidance request #\(guidanceRequestCount)")
        print("   Target: \(targetObject)")
        print("   Distance: \(arViewModel.distanceToObject?.description ?? "N/A")")
        print("   Time since last: \(String(format: "%.1f", timeSinceLastGuidance))s")
        print("   AI Speaking: \(geminiClient.isAISpeaking)")
        
        // **ë‹¨ìˆœí™”: ê¸°ë³¸ sendUserText ì‚¬ìš© (ë¹„ë””ì˜¤ í”„ë ˆì„ ìë™ í¬í•¨)**
        geminiClient.sendUserText(prompt)
        
        // âœ… ê°„ë‹¨í•œ pending í•´ì œ (íƒ€ì´ë¨¸ê°€ ë‹¤ìŒ ìš”ì²­ ê´€ë¦¬)
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            geminiClient.hasPendingGuidanceRequest = false
            print("ğŸ“ AppState: Guidance request pending status cleared")
        }
    }
    
    // MARK: - Helper Methods
    func setRequestedObject(_ objectName: String) {
        requestedObjectNameByUser = objectName.trimmingCharacters(in: .whitespacesAndNewlines)
        print("AppState: Set requested object to: '\(requestedObjectNameByUser)'")
    }
    
    func resetNavigation() {
        stopListeningForKeywords()
        stopPeriodicGuidance()
        transitionTo(.sttScanningMode)
    }
    
    // MARK: - State Queries
    var isInARMode: Bool {
        return currentStage == .sttScanningMode || currentStage == .liveGuidanceMode
    }
    
    var shouldShowDebugInfo: Bool {
        return isInARMode
    }
    
    var currentStageDescription: String {
        switch currentStage {
        case .sttScanningMode:
            if requestedObjectNameByUser.isEmpty {
                return "Stage 1: ìŒì„± ì¸ì‹ ëŒ€ê¸° ì¤‘" + (speechManager.isListening ? " ğŸ¤" : "")
            } else {
                return "Stage 1: '\(requestedObjectNameByUser)' ìŠ¤ìº” ì¤‘"
            }
        case .liveGuidanceMode:
            return "Stage 2: Live ê°€ì´ë˜ìŠ¤ ì¤‘"
        case .pureConversationMode:
            return "Stage 3: ììœ  ëŒ€í™” ëª¨ë“œ"
        }
    }
    
    // MARK: - AR Integration
    func setARViewModel(_ viewModel: ARViewModel) {
        self.arViewModel = viewModel
    }
    
    func setGeminiClient(_ client: GeminiLiveAPIClient) {
        self.geminiClient = client
        
        // âœ… Gemini Live API ì„¤ì • í›„ AudioManager ì´ˆê¸°í™”
        setupAudioManager()
        
        // âœ… AppState ì°¸ì¡° ì„¤ì • (Stage ì²´í¬ìš©)
        client.appState = self
        
        // Connect ARViewModel with GeminiClient for fresh frames
        if let arViewModel = arViewModel {
            client.arViewModel = arViewModel
            print("AppState: Connected GeminiClient with ARViewModel for fresh frames")
        }
    }
} 