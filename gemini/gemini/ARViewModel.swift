import SwiftUI
import ARKit
import Vision
import CoreHaptics
import RealityKit
import Accelerate

// FourCharCodeë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹° (ë””ë²„ê¹…ìš©)
extension FourCharCode {
    func toString() -> String {
        let cString: [CChar] = [
            CChar(self >> 24 & 0xFF),
            CChar(self >> 16 & 0xFF),
            CChar(self >> 8  & 0xFF),
            CChar(self >> 0  & 0xFF),
            0 // Null terminator
        ]
        return String(cString: cString)
    }
}

@MainActor
class ARViewModel: NSObject, ObservableObject, ARSessionDelegate {
    @Published var detectedObjectCenteredness: CGFloat = 0.0
    @Published var distanceToObject: Float? = nil
    @Published var raycastHitTransform: simd_float4x4? = nil
    @Published var detectedObjectLabels: [String] = []
    @Published var userTargetObjectName: String = "bed"
    @Published var lastDepthMap: CVPixelBuffer? = nil
    @Published var depthMapPreviewImage: Image? = nil
    
    // Scanning mode properties
    @Published var isScanningMode: Bool = false
    @Published var scanningTargetObject: String = ""
    @Published var scanProgress: Float = 0.0
    @Published var foundTarget: Bool = false
    
    // Haptic guidance properties
    @Published var isHapticGuideActive: Bool = false
    @Published var hapticGuidanceDirection: String = ""
    @Published var isTargetReached: Bool = false

    // âœ… ì¤‘ì•™ íƒì§€ ê°•í™”ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í”„ë¡œí¼í‹°ë“¤ (Stage 1â†’2 ì „í™˜ìš©)
    @Published var centerDetectionProgress: Float = 0.0 // ì¤‘ì•™ íƒì§€ ì§„í–‰ë¥  (0.0 ~ 1.0)
    @Published var isCenterDetectionActive: Bool = false // ì¤‘ì•™ íƒì§€ê°€ í™œì„±í™”ëœ ìƒíƒœ
    private let centerDetectionThreshold: CGFloat = 0.85 // 85% ì¤‘ì•™ íƒì§€ ì„ê³„ê°’ (Stage 1â†’2)

    // ARKit properties
    var arView: ARView?
    let session = ARSession()
    private let sceneReconstruction: ARConfiguration.SceneReconstruction = .mesh
    
    // âœ… ë¼ì´ë‹¤ ê¸°ë°˜ ì •í™•í•œ ê±°ë¦¬ ì¸¡ì •ì„ ìœ„í•œ ìƒˆë¡œìš´ í”„ë¡œí¼í‹°ë“¤
    @Published var lidarBasedDistance: Float? = nil // ë¼ì´ë‹¤ ì§ì ‘ ì¸¡ì • ê±°ë¦¬
    private var lastLidarProcessingTime = TimeInterval(0)
    private let lidarProcessingInterval: TimeInterval = 0.5 // ë¼ì´ë‹¤ ì²˜ë¦¬ ê°„ê²© (0.5ì´ˆ -> 0.2ì´ˆë¡œ ê°œì„ )

    // Vision properties
    private var segmentationModel: VNCoreMLModel?
    private var depthModel: VNCoreMLModel?
    private let visionQueue = DispatchQueue(label: "com.example.gemini.visionQueue", qos: .utility)
    private let confidenceThreshold: VNConfidence = 0.5
    
    // Haptics properties
    private var hapticEngine: CHHapticEngine?
    private var lastHapticTime: Date?
    private var debugSphere: ModelEntity?

    // Throttling properties
    private var lastFrameProcessingTime = TimeInterval(0)
    private let processingInterval: TimeInterval = 0.5 // Vision ì²˜ë¦¬ ê°„ê²© (0.1ì´ˆ -> 0.5ì´ˆë¡œ ì¦ê°€í•˜ì—¬ ARFrame retention ê°ì†Œ)
    
    // âœ… Gemini í”„ë ˆì„ ì „ì†¡ ì‹œê°„ ì¶”ì  ì¶”ê°€
    private var lastGeminiFrameTime = TimeInterval(0)
    
    // Gemini API Client
    weak var geminiClient: GeminiLiveAPIClient?
    
    // âœ… ë¡œë”© ê´€ë¦¬ì ì°¸ì¡° ì¶”ê°€
    weak var loadingManager: LoadingManager?

    // Class Labels for DETR model (ì›ë³¸ lidar test í”„ë¡œì íŠ¸ì˜ ë ˆì´ë¸” ì‚¬ìš©)
    private let detrClassLabels: [String] = {
        let labels = ["--", "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light", "fire hydrant", "--", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "--", "backpack", "umbrella", "--", "--", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "--", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch", "potted plant", "bed", "--", "dining table", "--", "--", "toilet", "--", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink", "refrigerator", "--", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush", "--", "banner", "blanket", "--", "bridge", "--", "--", "--", "--", "cardboard", "--", "--", "--", "--", "--", "--", "counter", "--", "curtain", "--", "--", "door", "--", "--", "--", "--", "--", "floor (wood)", "flower", "--", "--", "fruit", "--", "--", "gravel", "--", "--", "house", "--", "light", "--", "--", "mirror", "--", "--", "--", "--", "net", "--", "--", "pillow", "--", "--", "platform", "playingfield", "--", "railroad", "river", "road", "--", "roof", "--", "--", "sand", "sea", "shelf", "--", "--", "snow", "--", "stairs", "--", "--", "--", "--", "tent", "--", "towel", "--", "--", "wall (brick)", "--", "--", "--", "wall (stone)", "wall (tile)", "wall (wood)", "water (other)", "--", "window (blind)", "window (other)", "--", "--", "tree", "fence", "ceiling", "sky (other)", "cabinet", "table", "floor (other)", "pavement", "mountain", "grass", "dirt", "paper", "food (other)", "building (other)", "rock", "wall (other)", "rug"]
        
        // âœ… ë ˆì´ë¸” ë°°ì—´ ìœ íš¨ì„± ê²€ì¦
        guard !labels.isEmpty else {
            print("âŒ CRITICAL: detrClassLabels is empty! This will cause crashes.")
            return ["--", "unknown"] // ìµœì†Œí•œì˜ fallback
        }
        
        // âœ… ì˜ˆìƒ í¬ê¸° ê²€ì¦ (DETRëŠ” ë³´í†µ 80-200ê°œ í´ë˜ìŠ¤)
        guard labels.count > 50 else {
            print("âŒ WARNING: detrClassLabels count (\(labels.count)) seems too small")
            return labels // âœ… ëˆ„ë½ëœ return ì¶”ê°€
        }
        
        print("âœ… ARViewModel: detrClassLabels initialized with \(labels.count) classes")
        return labels
    }()

    // Object history accumulation
    @Published var allDetectedObjects: Set<String> = [] // ëˆ„ì ëœ ëª¨ë“  ê°ì²´ë“¤
    @Published var objectDetectionHistory: [(timestamp: Date, objects: [String])] = [] // ì‹œê°„ë³„ íˆìŠ¤í† ë¦¬
    private let maxHistoryEntries = 50 // ìµœëŒ€ íˆìŠ¤í† ë¦¬ ê°œìˆ˜

    // âœ… ì¶”ê°€: í–…í‹± ëª¨ë‹ˆí„°ë§ ì·¨ì†Œìš© ì‘ì—… ì¶”ì 
    private var hapticMonitoringTask: DispatchWorkItem?

    init(geminiClient: GeminiLiveAPIClient? = nil, loadingManager: LoadingManager? = nil) {
        self.geminiClient = geminiClient
        self.loadingManager = loadingManager
        super.init()
        loadVisionModels()
        setupHaptics()
    }

    // MARK: - AR Session Management
    func setupARSession() {
        // âœ… AR ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹œì‘ ì•Œë¦¼
        loadingManager?.updateProgress(step: 1, message: "AR ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
        
        guard ARWorldTrackingConfiguration.isSupported else {
            print("ARViewModel: ARWorldTrackingConfiguration is not supported on this device.")
            return
        }
        
        let configuration = ARWorldTrackingConfiguration()
        if ARWorldTrackingConfiguration.supportsSceneReconstruction(.mesh) {
            configuration.sceneReconstruction = sceneReconstruction
        } else {
            print("ARViewModel: Mesh reconstruction is not supported on this device for ARViewModel.")
        }
        configuration.planeDetection = [.horizontal, .vertical]
        
        if ARWorldTrackingConfiguration.supportsFrameSemantics(.personSegmentationWithDepth) {
            configuration.frameSemantics.insert(.personSegmentationWithDepth)
            // print("ARViewModel: Person segmentation with depth is supported and enabled.") // Already know this from logs
        } else {
            print("ARViewModel: Person segmentation with depth is not supported on this device for ARViewModel.")
        }

        session.delegate = self
        do {
            try session.run(configuration, options: [.resetTracking, .removeExistingAnchors])
            print("ARViewModel: ARSession.run() called successfully.") // More specific log
            
            // âœ… AR ì¹´ë©”ë¼ ì´ˆê¸°í™” ì™„ë£Œ ì•Œë¦¼
            loadingManager?.completeCurrentStep()
            
        } catch {
            print("ARViewModel: CRITICAL - ARSession.run() failed with error: \(error.localizedDescription)")
            // Consider setting an error state here to reflect in UI
        }
    }

    func pauseARSession() {
        session.pause()
        lastHapticTime = nil
        print("ARViewModel: ARSession paused.")
    }

    // MARK: - ARSessionDelegate
    func session(_ session: ARSession, didUpdate frame: ARFrame) {
        let currentTime = frame.timestamp
        
        // âœ… ARFrame ì°¸ì¡° ìµœì†Œí™”: í•„ìš”í•œ ë°ì´í„°ë§Œ ì¦‰ì‹œ ë³µì‚¬
        let pixelBuffer = frame.capturedImage
        let cameraTransform = frame.camera.transform
        
        // âœ… ë¼ì´ë‹¤ ê¸°ë°˜ ê±°ë¦¬ ì¸¡ì • (ì„±ëŠ¥ ê°œì„ : 0.5ì´ˆ ê°„ê²©)
        if currentTime - lastLidarProcessingTime >= lidarProcessingInterval {
            lastLidarProcessingTime = currentTime
            processLidarDistanceMeasurement(frame: frame)
        }
        
        // âœ… Geminië¡œ í”„ë ˆì„ ì „ì†¡: ì •í™•í•œ 1ì´ˆ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì •
        if let geminiClient = geminiClient, geminiClient.isConnected {
            // âœ… ê¸°ì¡´ ë¶€ì •í™•í•œ ì¡°ê±´ ì œê±°í•˜ê³  ì •í™•í•œ 1ì´ˆ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì •
            if currentTime - lastGeminiFrameTime >= 1.0 {
                lastGeminiFrameTime = currentTime
                
                // âœ… ì¦‰ì‹œ ë™ê¸°ì ìœ¼ë¡œ í”„ë ˆì„ ì „ì†¡ (ë¹„ë™ê¸° ì œê±°)
                geminiClient.sendVideoFrameImmediately(pixelBuffer: pixelBuffer)
            }
        }

        // âœ… Vision ì²˜ë¦¬ ì£¼ê¸° ì²´í¬ (0.5ì´ˆ ê°„ê²©)
        guard currentTime - lastFrameProcessingTime >= processingInterval else {
            return
        }
        lastFrameProcessingTime = currentTime
        
        // âœ… ARFrame ì—†ì´ í•„ìš”í•œ ë°ì´í„°ë§Œ ì „ë‹¬
        processFrameForVision(pixelBuffer: pixelBuffer, cameraTransform: cameraTransform, timestamp: currentTime)
    }

    func session(_ session: ARSession, didFailWithError error: Error) {
        print("ARViewModel: ARSession failed: \(error.localizedDescription)")
    }

    // **ì¶”ê°€: GeminiClientìš© ìµœì‹  í”„ë ˆì„ ì œê³µ ë©”ì„œë“œ**
    func getCurrentVideoFrameForGemini() -> String? {
        // âœ… í”„ë ˆì„ ìºì‹œë¥¼ í†µí•œ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
        guard let currentFrame = session.currentFrame else {
            return nil
        }
        
        // âœ… ì¦‰ì‹œ í•„ìš”í•œ ë°ì´í„°ë§Œ ë³µì‚¬í•˜ê³  ARFrame ì°¸ì¡° í•´ì œ
        let pixelBuffer = currentFrame.capturedImage
        
        // âœ… autoreleasepoolë¡œ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ + GeminiClientì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext ì‚¬ìš©
        return autoreleasepool {
            var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            ciImage = ciImage.oriented(.right)
            
            let targetScale: CGFloat = 0.5
            let scaleTransform = CGAffineTransform(scaleX: targetScale, y: targetScale)
            ciImage = ciImage.transformed(by: scaleTransform)
            
            // âœ… GeminiClientì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext ì‚¬ìš© (ì„±ëŠ¥ í–¥ìƒ)
            guard let jpegData = geminiClient?.reusableCIContext.jpegRepresentation(
                of: ciImage,
                colorSpace: ciImage.colorSpace ?? CGColorSpaceCreateDeviceRGB(),
                options: [kCGImageDestinationLossyCompressionQuality as CIImageRepresentationOption: 0.8]
            ) else {
                return nil
            }
            
            return jpegData.base64EncodedString()
        }
    }

    // MARK: - Vision Processing
    private func loadVisionModels() {
        // âœ… ë¡œë”© ì‹œì‘ ì•Œë¦¼
        loadingManager?.updateProgress(step: 2, message: "ê°ì²´ ì¸ì‹ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        // âœ… detrClassLabels ì‚¬ì „ ê²€ì¦
        guard !detrClassLabels.isEmpty else {
            print("âŒ CRITICAL: detrClassLabels is empty - Vision processing will be disabled")
            segmentationModel = nil
            depthModel = nil
            return
        }
        
        print("âœ… ARViewModel: detrClassLabels verified with \(detrClassLabels.count) classes")
        
        do {
            // âœ… DETR ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë”©
            guard let segModelURL = Bundle.main.url(forResource: "DETRResnet50SemanticSegmentationF16", withExtension: "mlmodelc") else {
                print("âŒ ARViewModel: DETR Segmentation model file not found.")
                segmentationModel = nil
                return
            }
            
            let segModel = try MLModel(contentsOf: segModelURL)
            segmentationModel = try VNCoreMLModel(for: segModel)
            print("âœ… ARViewModel: DETR Segmentation model loaded successfully.")
            
            // âœ… ê°ì²´ ì¸ì‹ ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì•Œë¦¼
            loadingManager?.completeCurrentStep()
            
            // âœ… ëª¨ë¸ ì¶œë ¥ ê²€ì¦
            let modelDescription = segModel.modelDescription
            print("ARViewModel: DETR Model info:")
            print("   Input: \(modelDescription.inputDescriptionsByName.keys.joined(separator: ", "))")
            print("   Output: \(modelDescription.outputDescriptionsByName.keys.joined(separator: ", "))")
            
        } catch {
            print("âŒ ARViewModel: Error loading DETR Segmentation model: \(error)")
            print("âŒ ARViewModel: Vision processing will be limited without segmentation model")
            segmentationModel = nil
        }

        // âœ… ê¹Šì´ ì¶”ì • ëª¨ë¸ ë¡œë”© ì‹œì‘ ì•Œë¦¼
        loadingManager?.updateProgress(step: 3, message: "ê¹Šì´ ì¶”ì • ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        do {
            // âœ… Depth ëª¨ë¸ ë¡œë”©
            guard let depthModelURL = Bundle.main.url(forResource: "DepthAnythingV2SmallF16P6", withExtension: "mlmodelc") else {
                print("âŒ ARViewModel: Depth Anything model file not found.")
                depthModel = nil
                return
            }
            
            let depthMLModel = try MLModel(contentsOf: depthModelURL)
            depthModel = try VNCoreMLModel(for: depthMLModel)
            print("âœ… ARViewModel: Depth Anything model loaded successfully.")
            
            // âœ… ê¹Šì´ ì¶”ì • ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì•Œë¦¼
            loadingManager?.completeCurrentStep()
            
        } catch {
            print("âŒ ARViewModel: Error loading Depth Anything model: \(error)")
            print("âŒ ARViewModel: Depth estimation will be unavailable")
            depthModel = nil
        }
        
        // âœ… ìµœì¢… ìƒíƒœ ê²€ì¦
        let segmentationAvailable = segmentationModel != nil
        let depthAvailable = depthModel != nil
        
        print("ARViewModel: Vision models status:")
        print("   - Segmentation: \(segmentationAvailable ? "âœ…" : "âŒ")")
        print("   - Depth: \(depthAvailable ? "âœ…" : "âŒ")")
        print("   - Class labels: \(detrClassLabels.count) classes")
        
        if !segmentationAvailable && !depthAvailable {
            print("âŒ ARViewModel: WARNING - No vision models available, app functionality will be severely limited")
        }
        
        // âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì•Œë¦¼
        loadingManager?.updateProgress(step: 4, message: "AI ìŒì„± ì‹œìŠ¤í…œ ì¤€ë¹„ ì¤‘...")
    }

    private func processFrameForVision(pixelBuffer: CVPixelBuffer, cameraTransform: simd_float4x4, timestamp: TimeInterval) {
        // âœ… detrClassLabels ë¨¼ì € ê²€ì¦
        guard !detrClassLabels.isEmpty else {
            print("âŒ ARViewModel: detrClassLabels is empty, skipping frame processing")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        guard let segModel = segmentationModel, let depthEstModel = depthModel else {
            // ë¡œê¹… ë¹ˆë„ ì¤„ì„ (ì„±ëŠ¥ ê°œì„ )
            let shouldLog = Int(timestamp * 10) % 50 == 0 // 5ì´ˆë§ˆë‹¤ í•œ ë²ˆë§Œ ë¡œê·¸
            if shouldLog {
                print("ARViewModel: Vision models not loaded, skipping frame processing.")
                print("   Segmentation model: \(segmentationModel != nil ? "âœ…" : "âŒ")")
                print("   Depth model: \(depthModel != nil ? "âœ…" : "âŒ")")
                print("   Class labels: \(detrClassLabels.count) classes")
            }
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }

        // âœ… ì„±ëŠ¥ ìµœì í™”: Vision ìš”ì²­ ì„¤ì •
        let segmentationRequest = VNCoreMLRequest(model: segModel) { [weak self] request, error in
            // âœ… ARFrame ë§¤ê°œë³€ìˆ˜ ì œê±°ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
            self?.processSegmentationResults(for: request, cameraTransform: cameraTransform, error: error)
        }
        segmentationRequest.imageCropAndScaleOption = .scaleFill
        // âœ… ì„±ëŠ¥ ê°œì„ : ì´ë¯¸ì§€ í¬ê¸° ì œí•œ
        segmentationRequest.usesCPUOnly = false // GPU ì‚¬ìš© í—ˆìš©

        let depthRequest = VNCoreMLRequest(model: depthEstModel) { [weak self] request, error in
            self?.processDepthResults(for: request, error: error)
        }
        depthRequest.imageCropAndScaleOption = .scaleFill
        depthRequest.usesCPUOnly = false // GPU ì‚¬ìš© í—ˆìš©

        // âœ… Vision ì²˜ë¦¬ë¥¼ ë¹„ë™ê¸° íì—ì„œ ì‹¤í–‰ (ìš°ì„ ìˆœìœ„ ë‚®ì¶¤)
        visionQueue.async {
            autoreleasepool { // âœ… ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ
                let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: .right, options: [:])
                do {
                    try handler.perform([segmentationRequest, depthRequest])
                } catch {
                    let shouldLogError = Int(Date().timeIntervalSince1970 * 10) % 50 == 0 // ì—ëŸ¬ ë¡œê¹…ë„ ì œí•œ
                    if shouldLogError {
                        print("âŒ ARViewModel: Failed to perform Vision requests: \(error)")
                    }
                    DispatchQueue.main.async { [weak self] in
                        self?.clearVisionResults()
                    }
                }
                // âœ… autoreleasepoolë¡œ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œë¨
            }
        }
    }
    
    private func clearVisionResults() {
        self.detectedObjectLabels = []
        self.detectedObjectCenteredness = 0.0
        self.distanceToObject = nil
        self.raycastHitTransform = nil
        self.lastDepthMap = nil
        self.depthMapPreviewImage = nil
        self.updateDebugSphere()
        self.updateHapticFeedback(centeredness: 0.0)
    }

    // MARK: - DETR Segmentation Results Processing
    private func processSegmentationResults(for request: VNRequest, cameraTransform: simd_float4x4, error: Error?) {
        guard error == nil else {
            print("ARViewModel: Error processing DETR segmentation: \(error!.localizedDescription)")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        guard let results = request.results as? [VNCoreMLFeatureValueObservation],
              let segmentationMap = results.first?.featureValue.multiArrayValue else {
            print("ARViewModel: Unexpected result type or could not get MultiArray from DETR model.")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        // âœ… detrClassLabels ë°°ì—´ ìœ íš¨ì„± ë¨¼ì € ê²€ì‚¬
        guard !detrClassLabels.isEmpty else {
            print("âŒ ARViewModel: detrClassLabels is empty - cannot process segmentation")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        let shapeDimensions = segmentationMap.shape.map { $0.intValue }
        guard shapeDimensions.count >= 2 else {
            print("âŒ ARViewModel: Invalid segmentation map shape")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        let height = shapeDimensions[shapeDimensions.count - 2]
        let width = shapeDimensions[shapeDimensions.count - 1]
        
        // âœ… ë©”ëª¨ë¦¬ í¬ê¸° ê²€ì¦ ë° ì•ˆì „ ì¥ì¹˜ ê°•í™”
        let expectedDataSize = height * width
        let actualDataSize = segmentationMap.count
        
        guard expectedDataSize == actualDataSize && expectedDataSize > 0 && height > 0 && width > 0 else {
            print("âŒ ARViewModel: Segmentation map size mismatch - expected: \(expectedDataSize), actual: \(actualDataSize), dims: \(width)x\(height)")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        // âœ… MLMultiArray ë°ì´í„° í¬ì¸í„° ì•ˆì „ ê²€ì‚¬
        guard segmentationMap.dataType == .int32,
              let dataPointer = try? segmentationMap.dataPointer.bindMemory(to: Int32.self, capacity: actualDataSize) else {
            print("âŒ ARViewModel: Cannot access segmentation data safely or wrong data type")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        var detectedClassIDs = Set<Int32>()
        var targetPixelCoordinates: [(x: Int, y: Int)] = []
        
        let targetObjectNameLowercased = self.userTargetObjectName.trimmingCharacters(in: .whitespacesAndNewlines).lowercased()
        var validTargetClassID: Int? = nil
        if !targetObjectNameLowercased.isEmpty {
            validTargetClassID = detrClassLabels.firstIndex(where: { $0.lowercased() == targetObjectNameLowercased })
        }

        // âœ… ë” ì•ˆì „í•œ í”½ì…€ ìˆœíšŒ ë° ê²€ì¦ with early exit
        let maxClassID = Int32(detrClassLabels.count - 1)
        
        // âœ… ë°°ì—´ ì ‘ê·¼ ì˜¤ë¥˜ ì™„ì „ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ê²€ì‚¬ ê°•í™”
        guard maxClassID >= 0 && detrClassLabels.count > 0 else {
            print("âŒ ARViewModel: Invalid detrClassLabels array state - count: \(detrClassLabels.count)")
            DispatchQueue.main.async { [weak self] in self?.clearVisionResults() }
            return
        }
        
        for y_coord in 0..<height {
            for x_coord in 0..<width {
                let index = y_coord * width + x_coord
                
                // âœ… ì¸ë±ìŠ¤ ê²½ê³„ ê²€ì‚¬ ê°•í™”
                guard index >= 0 && index < actualDataSize else {
                    continue // ë¡œê¹… ì œê±°í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
                }
                
                let classID = dataPointer[index]
                
                // âœ… classID ë²”ìœ„ ê²€ì‚¬ - ë” ì—„ê²©í•œ ê²€ì¦
                guard classID >= 0 && classID <= maxClassID else {
                    continue // ë¡œê¹… ì œê±°í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
                }

                // âœ… ë°°ì—´ ì ‘ê·¼ ì „ triple ì•ˆì „ ê²€ì‚¬
                let classIndex = Int(classID)
                guard classIndex >= 0 && 
                      classIndex < detrClassLabels.count && 
                      detrClassLabels.indices.contains(classIndex) else {
                    continue // ë¡œê¹… ì œê±°í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
                }
                
                // âœ… ì´ì œ ì™„ì „íˆ ì•ˆì „í•˜ê²Œ ë°°ì—´ ì ‘ê·¼
                let labelValue = detrClassLabels[classIndex]
                if classID > 0 && labelValue != "--" && !labelValue.isEmpty {
                    detectedClassIDs.insert(classID)
                }
                
                if let targetID = validTargetClassID, classID == Int32(targetID) {
                    targetPixelCoordinates.append((x: x_coord, y: y_coord))
                }
            }
        }
        
        // âœ… ì•ˆì „í•œ ë¼ë²¨ ë³€í™˜ - ì¶”ê°€ ê²€ì¦
        let finalLabels: [String] = detectedClassIDs.compactMap { id in
            let index = Int(id)
            // âœ… ì´ì¤‘ ê²€ì‚¬
            guard index >= 0 && index < detrClassLabels.count else { 
                print("âŒ ARViewModel: Invalid index \(index) for detrClassLabels (count: \(detrClassLabels.count))")
                return nil 
            }
            
            let label = detrClassLabels[index]
            guard label != "--" && !label.isEmpty else { 
                return nil 
            }
            return label
        }.sorted()

        var targetCenterPoint: CGPoint? = nil
        var currentCenteredness: CGFloat = 0.0

        if !targetPixelCoordinates.isEmpty && validTargetClassID != nil {
            let totalX = targetPixelCoordinates.reduce(0) { $0 + $1.x }
            let totalY = targetPixelCoordinates.reduce(0) { $0 + $1.y }
            let avgX = CGFloat(totalX) / CGFloat(targetPixelCoordinates.count)
            let avgY = CGFloat(totalY) / CGFloat(targetPixelCoordinates.count)
            
            // âœ… ì•ˆì „í•œ ë‚˜ëˆ„ê¸° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
            let safeWidth = max(1, width - 1)
            let safeHeight = max(1, height - 1)
            targetCenterPoint = CGPoint(x: avgX / CGFloat(safeWidth), y: avgY / CGFloat(safeHeight))

            if let center = targetCenterPoint {
                 let distanceToCenter = sqrt(pow(center.x - 0.5, 2) + pow(center.y - 0.5, 2))
                 currentCenteredness = max(0.0, 1.0 - (distanceToCenter / 0.707))
            }
        }
        
        // âœ… ì¤‘ì•™ íƒì§€ ê°•í™” ë¡œì§ ì¶”ê°€ (Stage 1â†’2 ì „í™˜ ê°ì§€ìš©)
        let isCurrentlyInCenter = currentCenteredness > self.centerDetectionThreshold
        
        // âœ… ê±°ë¦¬ ì¸¡ì •ì„ ìœ„í•œ raycastëŠ” ë³„ë„ ì¡°ê±´ìœ¼ë¡œ ì‹¤í–‰ (ë” ê´€ëŒ€í•œ ì¡°ê±´)
        let shouldExecuteRaycast = currentCenteredness > 0.7 && targetCenterPoint != nil && validTargetClassID != nil
        
        // âœ… raycast ê²°ê³¼ë¥¼ ìœ„í•œ ë³€ìˆ˜
        var targetHitTransform: simd_float4x4? = nil
        var targetDistance: Float? = nil
        
        DispatchQueue.main.async { [weak self] in
             guard let self = self else { return }
             
             // âœ… í˜„ì¬ í”„ë ˆì„ íƒì§€ ê²°ê³¼ ì—…ë°ì´íŠ¸
             self.detectedObjectLabels = finalLabels
             self.detectedObjectCenteredness = currentCenteredness
             
             // âœ… ëˆ„ì  ê°ì²´ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ê³ ìœ í•œ ê°ì²´ë“¤ë§Œ)
             let newObjects = Set(finalLabels)
             self.allDetectedObjects.formUnion(newObjects)
             
             // âœ… ì¤‘ì•™ íƒì§€ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (Stage 1â†’2 ì „í™˜ ê°ì§€ìš©)
             self.updateCenterDetectionProgress(isInCenter: isCurrentlyInCenter)
             
             // âœ… ê±°ë¦¬ ì¸¡ì •ì„ ìœ„í•œ raycast (Stage 1â†’2 ì „í™˜ê³¼ ë³„ê°œ)
             if shouldExecuteRaycast, let center = targetCenterPoint, let view = self.arView {
                 // âœ… ë¼ì´ë‹¤ ê±°ë¦¬ ì¸¡ì •ì€ ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” raycastë§Œ ì‹¤í–‰
                 let viewPoint = CGPoint(x: center.x * view.bounds.width, y: center.y * view.bounds.height)
                 
                 if let result = self.performRaycast(from: viewPoint, in: view) {
                     targetDistance = simd_distance(cameraTransform.columns.3.xyz, result.worldTransform.columns.3.xyz)
                     targetHitTransform = result.worldTransform
                     // âœ… ë¡œê¹… ë¹ˆë„ ì¤„ì„ (ì„±ëŠ¥ ê°œì„ )
                     let shouldLogRaycast = Int(Date().timeIntervalSince1970 * 10) % 30 == 0
                     if shouldLogRaycast {
                         print("ğŸ“ ARViewModel: Raycast distance: \(String(format: "%.2f", targetDistance ?? 0))m")
                     }
                 } else {
                     targetDistance = nil
                     targetHitTransform = nil
                 }
                 
                 // âœ… ë¼ì´ë‹¤ ê±°ë¦¬ê°€ ì´ë¯¸ ì¸¡ì •ë˜ì—ˆë‹¤ë©´ ë¼ì´ë‹¤ ìš°ì„  ì‚¬ìš©
                 if let lidarDistance = self.lidarBasedDistance {
                     targetDistance = lidarDistance
                     let shouldLogLidar = Int(Date().timeIntervalSince1970 * 10) % 30 == 0
                     if shouldLogLidar {
                         print("ğŸ“ ARViewModel: Using LiDAR distance: \(String(format: "%.2f", lidarDistance))m (overriding raycast)")
                     }
                 }
                 
                 self.distanceToObject = targetDistance
                 self.raycastHitTransform = targetHitTransform
                 self.updateDebugSphere()
             } else {
                 if self.distanceToObject != nil || self.raycastHitTransform != nil {
                    self.distanceToObject = nil
                    self.raycastHitTransform = nil
                    self.updateDebugSphere()
                 }
             }
             
             // **ìŠ¤ìº” ëª¨ë“œ ì¤‘ì¼ ë•Œ AppStateì˜ scannedObjectLabelsì— ëˆ„ì **
             if self.isScanningMode {
                 let newObjects = Set(finalLabels)
                 
                 // ê¸°ì¡´ allDetectedObjectsì— ëˆ„ì 
                 self.allDetectedObjects.formUnion(newObjects)
                 
                 // 360ë„ ìŠ¤ìº” ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì„ì‹œë¡œ ê°ì²´ ìˆ˜ ê¸°ë°˜)
                 let progress = min(1.0, Float(self.allDetectedObjects.count) / 10.0)
                 self.scanProgress = progress
                 
                 // íƒ€ê²Ÿ ê°ì²´ ë°œê²¬ ì—¬ë¶€ í™•ì¸
                 if !self.foundTarget && finalLabels.contains(where: { 
                     $0.lowercased() == self.scanningTargetObject.lowercased() 
                 }) {
                     self.foundTarget = true
                 }
             }
             
             if validTargetClassID == nil && !targetObjectNameLowercased.isEmpty {
                 // íƒ€ê²Ÿ ê°ì²´ê°€ ê°ì§€ ê°€ëŠ¥í•œ ë¦¬ìŠ¤íŠ¸ì— ì—†ìŒ
             }
             
             // âœ… í–…í‹± í”¼ë“œë°±ë„ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œ
             self.updateHapticFeedback(centeredness: currentCenteredness)
        }
    }

    // MARK: - Depth Anything Results Processing
    private func processDepthResults(for request: VNRequest, error: Error?) {
        guard error == nil else {
            print("ARViewModel: Error processing Depth Anything: \(error!.localizedDescription)")
            DispatchQueue.main.async { [weak self] in
                self?.lastDepthMap = nil
                self?.depthMapPreviewImage = nil
            }
            return
        }

        if let results = request.results as? [VNPixelBufferObservation], let depthMap = results.first?.pixelBuffer {
            DispatchQueue.main.async { [weak self] in
                self?.lastDepthMap = depthMap
                self?.updateDepthMapPreviewImage()
            }
        } else {
            print("ARViewModel: Could not process depth map from Depth Anything results or unexpected format.")
            DispatchQueue.main.async { [weak self] in
                self?.lastDepthMap = nil
                self?.depthMapPreviewImage = nil
            }
        }
    }

    // MARK: - Depth Map Visualization
    private func updateDepthMapPreviewImage() {
        guard let depthPixelBuffer = self.lastDepthMap else {
            self.depthMapPreviewImage = nil
            return
        }
        
        // âœ… ë”œë ˆì´ ê°œì„ : ë¹„ë™ê¸° ì²˜ë¦¬ ì œê±°í•˜ê³  ë°”ë¡œ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬
        if let cgImage = self.createGrayscaleCGImageFromDepthBuffer(depthPixelBuffer) {
            // âœ… ì›ë³¸ ê°€ë¡œ ë°©í–¥ ìœ ì§€ (íšŒì „ ì œê±°)
            self.depthMapPreviewImage = Image(decorative: cgImage, scale: 1.0, orientation: .up)
        } else {
            self.depthMapPreviewImage = nil
        }
    }

    private func createGrayscaleCGImageFromDepthBuffer(_ pixelBuffer: CVPixelBuffer) -> CGImage? {
        let width = CVPixelBufferGetWidth(pixelBuffer)
        let height = CVPixelBufferGetHeight(pixelBuffer)
        let pixelFormat = CVPixelBufferGetPixelFormatType(pixelBuffer)

        guard pixelFormat == kCVPixelFormatType_DepthFloat16 || pixelFormat == kCVPixelFormatType_OneComponent16Half ||
              pixelFormat == kCVPixelFormatType_DepthFloat32 || pixelFormat == kCVPixelFormatType_OneComponent32Float else {
            print("ARViewModel: Unsupported pixel format for depth map: \(pixelFormat.toString())")
            return nil
        }
        
        // âœ… ìµœì†Œ í¬ê¸° ê²€ì¦ ì¶”ê°€
        guard width > 0 && height > 0 else {
            print("ARViewModel: Invalid depth buffer dimensions: \(width)x\(height)")
            return nil
        }
        
        CVPixelBufferLockBaseAddress(pixelBuffer, .readOnly)
        defer { CVPixelBufferUnlockBaseAddress(pixelBuffer, .readOnly) }

        guard let baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) else {
            print("ARViewModel: Failed to get base address from CVPixelBuffer for depth.")
            return nil
        }
        
        let expectedBytes = CVPixelBufferGetBytesPerRow(pixelBuffer) * height
        guard expectedBytes > 0 else {
            print("ARViewModel: Invalid depth buffer byte size: \(expectedBytes)")
            return nil
        }
        
        var srcBuffer = vImage_Buffer(data: baseAddress,
                                      height: vImagePixelCount(height),
                                      width: vImagePixelCount(width),
                                      rowBytes: CVPixelBufferGetBytesPerRow(pixelBuffer))
        
        var float32PlanarBuffer: vImage_Buffer
        var allocatedFloat32Data: UnsafeMutableRawPointer? = nil
        
        let dataSize = width * height * MemoryLayout<Float32>.size
        allocatedFloat32Data = malloc(dataSize)
        guard let validAllocatedData = allocatedFloat32Data else {
            print("ARViewModel: Failed to allocate memory for Float32 depth buffer.")
            return nil
        }
        float32PlanarBuffer = vImage_Buffer(data: validAllocatedData,
                                            height: vImagePixelCount(height),
                                            width: vImagePixelCount(width),
                                            rowBytes: width * MemoryLayout<Float32>.size)

        // âœ… vImage ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        var conversionSuccess = false
        if pixelFormat == kCVPixelFormatType_DepthFloat16 || pixelFormat == kCVPixelFormatType_OneComponent16Half {
            let conversionError = vImageConvert_Planar16FtoPlanarF(&srcBuffer, &float32PlanarBuffer, 0)
            if conversionError == kvImageNoError {
                conversionSuccess = true
            } else {
                print("ARViewModel: vImageConvert_Planar16FtoPlanarF error: \(conversionError)")
            }
        } else { // Float32 formats
            if srcBuffer.rowBytes == width * MemoryLayout<Float32>.size {
                 memcpy(float32PlanarBuffer.data, srcBuffer.data, dataSize)
                 conversionSuccess = true
             } else {
                 for y_coord in 0..<height {
                     let srcRow = srcBuffer.data!.advanced(by: y_coord * srcBuffer.rowBytes)
                     let dstRow = float32PlanarBuffer.data!.advanced(by: y_coord * float32PlanarBuffer.rowBytes)
                     memcpy(dstRow, srcRow, width * MemoryLayout<Float32>.size)
                 }
                 conversionSuccess = true
             }
        }
        
        guard conversionSuccess else {
            free(allocatedFloat32Data)
            print("ARViewModel: Failed to convert depth buffer to Float32")
            return nil
        }
        
        var minPixelVal: Float = 0.0
        var maxPixelVal: Float = 1.0
        
        // âœ… ì™„ì „íˆ ì•ˆì „í•œ í¬ì¸í„° ì ‘ê·¼
        guard let dataPtr = float32PlanarBuffer.data?.assumingMemoryBound(to: Float32.self) else {
            free(allocatedFloat32Data)
            print("ARViewModel: Failed to bind memory to Float32 pointer")
            return nil
        }
        
        let pixelCount = width * height
        guard pixelCount > 0 else {
            free(allocatedFloat32Data)
            print("ARViewModel: Zero pixel count")
            return nil
        }
        
        // âœ… ë©”ëª¨ë¦¬ ê²½ê³„ ê²€ì‚¬ë¥¼ í†µí•œ ì•ˆì „í•œ ì ‘ê·¼
        var validPixelFound = false
        for i in 0..<pixelCount {
            let pixelValue = dataPtr[i]
            if !pixelValue.isNaN && !pixelValue.isInfinite && pixelValue >= 0 {
                if !validPixelFound {
                    minPixelVal = pixelValue
                    maxPixelVal = pixelValue
                    validPixelFound = true
                } else {
                    if pixelValue < minPixelVal { minPixelVal = pixelValue }
                    if pixelValue > maxPixelVal { maxPixelVal = pixelValue }
                }
            }
        }
        
        if !validPixelFound {
            free(allocatedFloat32Data)
            print("ARViewModel: No valid depth pixels found")
            return nil
        }
        
        if maxPixelVal <= minPixelVal { maxPixelVal = minPixelVal + 1.0 }
        
        var destGrayscaleBuffer: vImage_Buffer
        let grayscaleDataSize = width * height * MemoryLayout<UInt8>.size
        guard let allocatedGrayscaleData = malloc(grayscaleDataSize) else {
            free(allocatedFloat32Data)
            print("ARViewModel: Failed to allocate memory for UInt8 depth buffer.")
            return nil
        }
        destGrayscaleBuffer = vImage_Buffer(data: allocatedGrayscaleData,
                                            height: vImagePixelCount(height),
                                            width: vImagePixelCount(width),
                                            rowBytes: width * MemoryLayout<UInt8>.size)

        let conversionErrorUInt8 = vImageConvert_PlanarFtoPlanar8(&float32PlanarBuffer, &destGrayscaleBuffer, minPixelVal, maxPixelVal, vImage_Flags(kvImageNoFlags))
        guard conversionErrorUInt8 == kvImageNoError else {
            print("ARViewModel: vImageConvert_PlanarFtoPlanar8 error: \(conversionErrorUInt8)")
            free(allocatedGrayscaleData)
            free(allocatedFloat32Data)
            return nil
        }

        // âœ… Float32 ë©”ëª¨ë¦¬ í•´ì œ
        free(allocatedFloat32Data)

        guard let provider = CGDataProvider(dataInfo: allocatedGrayscaleData,
                                            data: destGrayscaleBuffer.data!,
                                            size: grayscaleDataSize,
                                            releaseData: { info, _, size in free(info) }) else {
            print("ARViewModel: Failed to create CGDataProvider for depth image.")
            free(allocatedGrayscaleData)
            return nil
        }

        return CGImage(width: width, height: height, bitsPerComponent: 8, bitsPerPixel: 8,
                       bytesPerRow: destGrayscaleBuffer.rowBytes,
                       space: CGColorSpaceCreateDeviceGray(),
                       bitmapInfo: CGBitmapInfo(rawValue: CGImageAlphaInfo.none.rawValue),
                       provider: provider, decode: nil, shouldInterpolate: true, intent: .defaultIntent)
    }

    // MARK: - LiDAR Raycasting
    private func performRaycast(from point: CGPoint, in view: ARView) -> ARRaycastResult? {
        guard view.window != nil else { return nil }
        return view.raycast(from: point, allowing: .estimatedPlane, alignment: .any).first
    }
    
    // âœ… ìƒˆë¡œìš´ ë©”ì„œë“œ: ë¼ì´ë‹¤ ê¸°ë°˜ ì •í™•í•œ ê±°ë¦¬ ì¸¡ì •
    private func processLidarDistanceMeasurement(frame: ARFrame) {
        // íƒ€ê²Ÿ ê°ì²´ê°€ ì¤‘ì•™ì— ì–´ëŠ ì •ë„ ìœ„ì¹˜í•´ì•¼ ê±°ë¦¬ ì¸¡ì • ì‹œì‘
        guard detectedObjectCenteredness > 0.3 else {
            lidarBasedDistance = nil
            return
        }
        
        // âœ… sceneDepth ì§€ì› ì—¬ë¶€ ë¨¼ì € í™•ì¸
        guard ARWorldTrackingConfiguration.supportsSceneReconstruction(.mesh) else {
            // ë¼ì´ë‹¤ ë¯¸ì§€ì› ê¸°ê¸°ì—ì„œëŠ” raycast ì‚¬ìš©
            fallbackToRaycastDistance(frame: frame)
            return
        }
        
        // âœ… ARFrameì˜ ì‹¤ì œ depth ë°ì´í„° í™œìš©
        guard let depthData = frame.sceneDepth?.depthMap else {
            // print("ARViewModel: No sceneDepth data available, using raycast") // ë¡œê·¸ ê°„ì†Œí™”
            fallbackToRaycastDistance(frame: frame)
            return
        }
        
        // âœ… í™”ë©´ ì¤‘ì‹¬ì ì—ì„œì˜ depth ê°’ ì¶”ì¶œ
        let depthWidth = CVPixelBufferGetWidth(depthData)
        let depthHeight = CVPixelBufferGetHeight(depthData)
        
        // í™”ë©´ ì¤‘ì‹¬ì  ê³„ì‚°
        let centerX = depthWidth / 2
        let centerY = depthHeight / 2
        
        // âœ… depth ë°ì´í„°ì—ì„œ ì‹¤ì œ ê±°ë¦¬ ê°’ ì¶”ì¶œ
        if let actualDistance = extractDepthValue(from: depthData, at: CGPoint(x: centerX, y: centerY)) {
            DispatchQueue.main.async { [weak self] in
                self?.lidarBasedDistance = actualDistance
                // ê¸°ì¡´ distanceToObjectë„ ì—…ë°ì´íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
                self?.distanceToObject = actualDistance
                print("ğŸ“ ARViewModel: LiDAR distance to center: \(String(format: "%.2f", actualDistance))m")
            }
        } else {
            // ë¼ì´ë‹¤ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ raycast ì‚¬ìš©
            fallbackToRaycastDistance(frame: frame)
        }
    }
    
    // âœ… ë¼ì´ë‹¤ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ raycast ë°©ì‹ìœ¼ë¡œ fallback
    private func fallbackToRaycastDistance(frame: ARFrame) {
        guard let arView = arView, 
              detectedObjectCenteredness > 0.7 else {
            return
        }
        
        // í™”ë©´ ì¤‘ì‹¬ì ì—ì„œ raycast ì‹¤í–‰
        let centerPoint = CGPoint(x: arView.bounds.width / 2, y: arView.bounds.height / 2)
        
        if let result = performRaycast(from: centerPoint, in: arView) {
            let cameraTransform = frame.camera.transform
            let raycastDistance = simd_distance(cameraTransform.columns.3.xyz, result.worldTransform.columns.3.xyz)
            
            DispatchQueue.main.async { [weak self] in
                self?.lidarBasedDistance = raycastDistance
                self?.distanceToObject = raycastDistance
                print("ğŸ“ ARViewModel: Raycast fallback distance: \(String(format: "%.2f", raycastDistance))m")
            }
        }
    }

    // MARK: - Debug Visualization
    private func updateDebugSphere() {
        guard let arView = arView else { return }

        // âœ… ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ ê¸°ì¡´ êµ¬ì²´ë¥¼ í™•ì‹¤íˆ ì œê±°
        if let oldSphere = self.debugSphere {
            // ê¸°ì¡´ ì•µì»¤ ì°¾ì•„ì„œ ì œê±°
            let anchorsContainingOldSphere = arView.scene.anchors.filter { anchorEntity in
                anchorEntity.children.contains(oldSphere)
            }
            for anchor in anchorsContainingOldSphere {
                arView.scene.removeAnchor(anchor)
            }
            self.debugSphere = nil
        }

        // âœ… ìƒˆë¡œìš´ ìœ íš¨í•œ hitTransformì´ ìˆìœ¼ë©´ êµ¬ì²´ ìƒì„± ë° ì¶”ê°€
        if let hitTransform = raycastHitTransform {
            // âœ… ì˜ˆì‹œ ì½”ë“œì²˜ëŸ¼ ë” ì‘ì€ ë°˜ì§€ë¦„ê³¼ ë¹¨ê°„ìƒ‰ ì‚¬ìš©
            let sphereMesh = MeshResource.generateSphere(radius: 0.01)
            let sphereMaterial = SimpleMaterial(color: .red, isMetallic: false)
            let sphereEntity = ModelEntity(mesh: sphereMesh, materials: [sphereMaterial])
            
            let newAnchor = AnchorEntity(world: hitTransform)
            newAnchor.addChild(sphereEntity)
            arView.scene.addAnchor(newAnchor)
            self.debugSphere = sphereEntity
            
            print("ARViewModel: Debug sphere created at target position")
        } else {
            print("ARViewModel: Debug sphere removed (no valid hit transform)")
        }
    }

    // MARK: - Haptics
    private func setupHaptics() {
        guard CHHapticEngine.capabilitiesForHardware().supportsHaptics else { return }
        do {
            hapticEngine = try CHHapticEngine()
            try hapticEngine?.start()
            hapticEngine?.playsHapticsOnly = true
            hapticEngine?.stoppedHandler = { reason in
                print("ARViewModel: Haptic engine stopped for reason: \(reason.rawValue)")
            }
            hapticEngine?.resetHandler = { [weak self] in
                print("ARViewModel: Haptic engine reset. Attempting to restart.")
                do {
                    try self?.hapticEngine?.start()
                } catch {
                    print("ARViewModel: Failed to restart haptic engine after reset: \(error)")
                }
            }
            print("ARViewModel: Haptic engine started.")
        } catch {
            print("ARViewModel: Error starting haptic engine: \(error.localizedDescription)")
        }
    }

    private func updateHapticFeedback(centeredness: CGFloat) {
        // âœ… í–…í‹± ê°€ì´ë“œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë°”ë¡œ ë¦¬í„´
        guard isHapticGuideActive else { 
            return 
        }
        
        guard let engine = hapticEngine else { return }

        let now = Date()
        // âœ… ì ‘ê·¼ì„±ì„ ìœ„í•´ ë” ë¹ ë¥¸ ë°˜ì‘ (0.08ì´ˆ)
        if let lastTime = lastHapticTime, now.timeIntervalSince(lastTime) < 0.08 {
            return
        }

        // âœ… ì ‘ê·¼ì„± ìš°ì„ : í›¨ì”¬ ë” ë„“ì€ ë²”ìœ„ì™€ ë§ì€ ë‹¨ê³„ì˜ í–…í‹± í”¼ë“œë°±
        let intensityValue: Float
        let sharpnessValue: Float
        let shouldProvideHaptic: Bool
        let feedbackDescription: String

        if centeredness > 0.9 {
            // âœ… ì™„ë²½í•œ ì¤‘ì‹¬ - ìµœê°• í”¼ë“œë°±
            intensityValue = 1.0
            sharpnessValue = 1.0
            shouldProvideHaptic = true
            feedbackDescription = "ğŸ¯ ì™„ë²½í•œ ì¤‘ì‹¬!"
            hapticGuidanceDirection = "ì™„ë²½í•œ ì¤‘ì‹¬!"
        } else if centeredness > 0.8 {
            // âœ… ë§¤ìš° ì¢‹ì€ ìœ„ì¹˜ - ê°•í•œ í”¼ë“œë°±
            intensityValue = 0.85
            sharpnessValue = 0.8
            shouldProvideHaptic = true
            feedbackDescription = "ğŸ¯ ê±°ì˜ ì™„ë²½!"
            hapticGuidanceDirection = "ê±°ì˜ ì™„ë²½í•œ ìœ„ì¹˜"
        } else if centeredness > 0.7 {
            // âœ… ì¢‹ì€ ìœ„ì¹˜ - ì¤‘ê°• í”¼ë“œë°±
            intensityValue = 0.7
            sharpnessValue = 0.6
            shouldProvideHaptic = true
            feedbackDescription = "âœ… ì¢‹ì€ ë°©í–¥"
            hapticGuidanceDirection = "ì¢‹ì€ ë°©í–¥ì…ë‹ˆë‹¤"
        } else if centeredness > 0.55 {
            // âœ… ê´œì°®ì€ ìœ„ì¹˜ - ì¤‘ê°„ í”¼ë“œë°±
            intensityValue = 0.55
            sharpnessValue = 0.45
            shouldProvideHaptic = true
            feedbackDescription = "ğŸ‘ ê´œì°®ì€ ë°©í–¥"
            hapticGuidanceDirection = "ì¡°ê¸ˆ ë” ì¡°ì •í•˜ì„¸ìš”"
        } else if centeredness > 0.4 {
            // âœ… ì•½ê°„ ë§ëŠ” ë°©í–¥ - ì•½í•œ í”¼ë“œë°±
            intensityValue = 0.4
            sharpnessValue = 0.3
            shouldProvideHaptic = true
            feedbackDescription = "ğŸ“ ì•½ê°„ ë§ëŠ” ë°©í–¥"
            hapticGuidanceDirection = "ë°©í–¥ ì¡°ì •ì´ í•„ìš”í•´ìš”"
        } else if centeredness > 0.25 {
            // âœ… í¬ë¯¸í•œ ê°ì§€ - ë§¤ìš° ì•½í•œ í”¼ë“œë°±
            intensityValue = 0.25
            sharpnessValue = 0.2
            shouldProvideHaptic = true
            feedbackDescription = "ğŸ‘€ ë¬¼ì²´ê°€ í™”ë©´ì— ë³´ì—¬ìš”"
            hapticGuidanceDirection = "ë¬¼ì²´ê°€ ë³´ì…ë‹ˆë‹¤"
        } else if centeredness > 0.1 {
            // âœ… ë¬¼ì²´ ê°ì§€ë¨ - ìµœì•½ í”¼ë“œë°±
            intensityValue = 0.15
            sharpnessValue = 0.1
            shouldProvideHaptic = true
            feedbackDescription = "ğŸ” ë¬¼ì²´ ê°ì§€ë¨"
            hapticGuidanceDirection = "ë¬¼ì²´ë¥¼ ì°¾ì•˜ì–´ìš”"
        } else {
            // âœ… ë¬¼ì²´ ì—†ìŒ - í–…í‹± ì—†ìŒ
            shouldProvideHaptic = false
            lastHapticTime = nil
            hapticGuidanceDirection = "ì²œì²œíˆ ë‘˜ëŸ¬ë³´ì„¸ìš”"
            return
        }
        
        guard shouldProvideHaptic else { return }
        
        do {
            // âœ… ì„¸ë°€í•œ í–…í‹± íŒ¨í„´ìœ¼ë¡œ ë” ì •í™•í•œ í”¼ë“œë°± ì œê³µ
            let intensity = CHHapticEventParameter(parameterID: .hapticIntensity, value: intensityValue)
            let sharpness = CHHapticEventParameter(parameterID: .hapticSharpness, value: sharpnessValue)
            let event = CHHapticEvent(eventType: .hapticTransient, parameters: [intensity, sharpness], relativeTime: 0)
            let pattern = try CHHapticPattern(events: [event], parameters: [])
            let player = try engine.makePlayer(with: pattern)
            try player.start(atTime: 0)
            lastHapticTime = now
            
        } catch {
            print("âŒ ARViewModel: Haptic error: \(error.localizedDescription)")
            if let chError = error as? CHHapticError {
                switch chError.code {
                case .engineNotRunning,
                     .resourceNotAvailable,
                     .notSupported,
                     .operationNotPermitted,
                     .invalidAudioSession:
                    setupHaptics()
                default:
                    setupHaptics()
                }
            } else {
                setupHaptics()
            }
        }
    }
    
    deinit {
        hapticEngine?.stop()
        hapticMonitoringTask?.cancel() // âœ… í–…í‹± ëª¨ë‹ˆí„°ë§ ì‘ì—… ì·¨ì†Œ
        
        print("ARViewModel deinitialized, haptic engine stopped.")
    }

    // MARK: - Scanning Mode Control
    func startScanning(for objectName: String) {
        print("ARViewModel: Starting scan mode for: '\(objectName)'")
        isScanningMode = true
        scanningTargetObject = objectName
        scanProgress = 0.0
        foundTarget = false
        userTargetObjectName = objectName // Update the existing target property
        
        // **ì¤‘ìš”: í•œêµ­ì–´ ê°ì²´ëª…ì¸ ê²½ìš° ì˜ì–´ ë§¤ì¹­ í•„ìš”ì„±ì„ ë¡œê·¸ë¡œ ì•Œë¦¼**
        let containsKorean = objectName.range(of: "[ã„±-ã…ã…-ã…£ê°€-í£]", options: .regularExpression) != nil
        if containsKorean {
            print("ARViewModel: Warning - Korean object name '\(objectName)' detected. This should be translated to English via Gemini API first.")
        }
        
        // ê°ì§€ ê°€ëŠ¥í•œ ì˜ì–´ ê°ì²´ëª…ì¸ì§€ í™•ì¸
        let objectNameLower = objectName.lowercased()
        let canDetectDirectly = detrClassLabels.contains { $0.lowercased() == objectNameLower }
        if canDetectDirectly {
            print("ARViewModel: Object '\(objectName)' found in DETR class labels - direct detection possible")
        } else {
            print("ARViewModel: Object '\(objectName)' not in DETR class labels - Gemini API matching will be required")
        }
        
        // Start scanning progress simulation
        simulateScanProgress()
    }
    
    func stopScanning() {
        print("ARViewModel: Stopping scan mode")
        isScanningMode = false
        scanningTargetObject = ""
        scanProgress = 0.0
        foundTarget = false
    }
    
    private func simulateScanProgress() {
        guard isScanningMode else { return }
        
        // Simulate scanning progress over 5 seconds
        let progressIncrement: Float = 0.1 // 10% increments
        let intervalTime: TimeInterval = 0.5 // 500ms intervals
        
        DispatchQueue.main.asyncAfter(deadline: .now() + intervalTime) {
            guard self.isScanningMode else { return }
            
            self.scanProgress += progressIncrement
            
            // Check if we found the target object during scanning
            if !self.foundTarget && self.detectedObjectLabels.contains(where: { 
                $0.lowercased().contains(self.scanningTargetObject.lowercased()) 
            }) {
                self.foundTarget = true
                print("ARViewModel: Target object '\(self.scanningTargetObject)' found during scan!")
            }
            
            if self.scanProgress < 1.0 {
                self.simulateScanProgress() // Continue scanning
            } else {
                // Scanning completed
                print("ARViewModel: Scan completed. Found target: \(self.foundTarget)")
                self.completeScan()
            }
        }
    }
    
    private func completeScan() {
        print("ARViewModel: Scan completion - Target found: \(foundTarget)")
        
        // Notify completion through delegate or callback mechanism
        // For now, we'll let AppState monitor the scanning completion
    }

    // MARK: - Haptic Guidance Control
    func startHapticGuidance(for objectName: String) {
        print("ARViewModel: Starting haptic guidance for: '\(objectName)'")
        isHapticGuideActive = true
        hapticGuidanceDirection = ""
        isTargetReached = false
        userTargetObjectName = objectName
        
        // Start haptic guidance monitoring
        startHapticGuidanceMonitor()
    }
    
    func stopHapticGuidance() {
        print("ARViewModel: Stopping haptic guidance")
        isHapticGuideActive = false
        hapticGuidanceDirection = ""
        isTargetReached = false
        
        // âœ… ì¤‘ìš”: ìŠ¤ì¼€ì¤„ëœ í–…í‹± ëª¨ë‹ˆí„°ë§ ì‘ì—… ì·¨ì†Œ
        hapticMonitoringTask?.cancel()
        hapticMonitoringTask = nil
        
        // âœ… ì¶”ê°€: í–…í‹± ì—”ì§„ ì™„ì „ ì¤‘ë‹¨
        hapticEngine?.stop()
        lastHapticTime = nil
        
        print("ARViewModel: âœ… Cancelled scheduled haptic monitoring tasks")
        print("ARViewModel: âœ… Stopped haptic engine completely")
    }
    
    private func startHapticGuidanceMonitor() {
        guard isHapticGuideActive else { return }
        
        // âœ… ê¸°ì¡´ ì‘ì—…ì´ ìˆìœ¼ë©´ ì·¨ì†Œ
        hapticMonitoringTask?.cancel()
        
        // âœ… ìƒˆë¡œìš´ ì·¨ì†Œ ê°€ëŠ¥í•œ ì‘ì—… ìƒì„±
        let workItem = DispatchWorkItem { [weak self] in
            guard let self = self, self.isHapticGuideActive else { 
                print("ARViewModel: Haptic monitoring cancelled or guide inactive")
                return 
            }
            
            let centeredness = self.detectedObjectCenteredness
            let distance = self.distanceToObject
            
            // âœ… ë” ë„“ì€ ë²”ìœ„ì—ì„œ ì„¸ë°€í•œ ê°€ì´ë˜ìŠ¤ ì œê³µ
            if let actualDistance = distance, 
               centeredness > 0.85 && actualDistance < 0.5 {
                // âœ… íƒ€ê²Ÿ ë„ë‹¬
                self.isTargetReached = true
                self.hapticGuidanceDirection = "ğŸ¯ ëª©í‘œ ë„ë‹¬!"
                
                // âœ… ì„±ê³µ í–…í‹± (3ë²ˆ ì—°ì† ê°•í•œ ì§„ë™)
                do {
                    if let engine = self.hapticEngine {
                        var events: [CHHapticEvent] = []
                        for i in 0..<3 {
                            let intensity = CHHapticEventParameter(parameterID: .hapticIntensity, value: 1.0)
                            let sharpness = CHHapticEventParameter(parameterID: .hapticSharpness, value: 1.0)
                            let event = CHHapticEvent(eventType: .hapticTransient, parameters: [intensity, sharpness], relativeTime: TimeInterval(i) * 0.2)
                            events.append(event)
                        }
                        let pattern = try CHHapticPattern(events: events, parameters: [])
                        let player = try engine.makePlayer(with: pattern)
                        try player.start(atTime: 0)
                    }
                } catch {
                    // í–…í‹± ì—ëŸ¬ ë¬´ì‹œ
                }
                
                return
            } else if centeredness > 0.7 {
                // âœ… ë§¤ìš° ì¢‹ì€ ë°©í–¥ - ì§ì§„
                self.hapticGuidanceDirection = "ğŸš€ ì§ì§„í•˜ì„¸ìš”!"
                self.isTargetReached = false
                
            } else if centeredness > 0.55 {
                // âœ… ì¢‹ì€ ë°©í–¥ - ê³„ì† ì§„í–‰
                self.hapticGuidanceDirection = "âœ… ì¢‹ì€ ë°©í–¥ì´ì—ìš”"
                self.isTargetReached = false
                
            } else if centeredness > 0.4 {
                // âœ… ê´œì°®ì€ ë°©í–¥ - ì¡°ê¸ˆ ì¡°ì •
                self.hapticGuidanceDirection = "ğŸ“ ì¡°ê¸ˆ ë” ì¡°ì •í•˜ì„¸ìš”"
                self.isTargetReached = false
                
            } else if centeredness > 0.25 {
                // âœ… ë¬¼ì²´ê°€ ë³´ì„ - ë°©í–¥ ì¡°ì •
                self.hapticGuidanceDirection = "ğŸ‘€ ë¬¼ì²´ê°€ í™”ë©´ì— ìˆì–´ìš”"
                self.isTargetReached = false
                
            } else if centeredness > 0.1 {
                // âœ… ë¬¼ì²´ ê°ì§€ë¨ - ë” í° ì¡°ì •
                self.hapticGuidanceDirection = "ğŸ” ë¬¼ì²´ë¥¼ ê°ì§€í–ˆì–´ìš”"
                self.isTargetReached = false
                
            } else {
                // âœ… ë¬¼ì²´ ì—†ìŒ - ë‘˜ëŸ¬ë³´ê¸°
                self.hapticGuidanceDirection = "ğŸ”„ ì²œì²œíˆ ë‘˜ëŸ¬ë³´ì„¸ìš”"
                self.isTargetReached = false
            }
            
            // âœ… ë” ë¹ ë¥¸ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë°˜ì‘ì„± í–¥ìƒ
            self.startHapticGuidanceMonitor()
        }
        
        // âœ… ë” ì§§ì€ ê°„ê²©ìœ¼ë¡œ ì¦‰ê°ì ì¸ í”¼ë“œë°± (0.15ì´ˆ)
        hapticMonitoringTask = workItem
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.15, execute: workItem)
    }

    // MARK: - Center Detection Management
    private func updateCenterDetectionProgress(isInCenter: Bool) {
        if isInCenter {
            // âœ… 85% ì¤‘ì•™ì— ë“¤ì–´ê°€ë©´ ë°”ë¡œ í™œì„±í™” (1ì´ˆ ìœ ì§€ ì¡°ê±´ ì œê±°)
            centerDetectionProgress = 1.0
            if !isCenterDetectionActive {
                isCenterDetectionActive = true
                print("âœ… ARViewModel: Center detection activated immediately! (85% threshold met)")
            }
        } else {
            // âœ… ì¤‘ì•™ì—ì„œ ë²—ì–´ë‚œ ê²½ìš° - ë¦¬ì…‹
            if isCenterDetectionActive {
                print("âŒ ARViewModel: Center detection reset (moved away from center)")
                resetCenterDetection()
            }
        }
    }
    
    private func resetCenterDetection() {
        centerDetectionProgress = 0.0
        isCenterDetectionActive = false
    }

    // âœ… Depth ë²„í¼ì—ì„œ íŠ¹ì • ì¢Œí‘œì˜ ê¹Šì´ ê°’ ì¶”ì¶œ
    private func extractDepthValue(from depthBuffer: CVPixelBuffer, at point: CGPoint) -> Float? {
        CVPixelBufferLockBaseAddress(depthBuffer, .readOnly)
        defer { CVPixelBufferUnlockBaseAddress(depthBuffer, .readOnly) }
        
        let width = CVPixelBufferGetWidth(depthBuffer)
        let height = CVPixelBufferGetHeight(depthBuffer)
        
        let x = Int(point.x)
        let y = Int(point.y)
        
        // ê²½ê³„ ê²€ì‚¬ ê°•í™”
        guard x >= 0 && x < width && y >= 0 && y < height else {
            return nil
        }
        
        let pixelFormat = CVPixelBufferGetPixelFormatType(depthBuffer)
        
        guard let baseAddress = CVPixelBufferGetBaseAddress(depthBuffer) else {
            return nil
        }
        
        let bytesPerRow = CVPixelBufferGetBytesPerRow(depthBuffer)
        
        // âœ… í”½ì…€ í¬ë§·ì— ë”°ë¥¸ depth ê°’ ì¶”ì¶œ (ë” ì•ˆì „í•œ ë°©ì‹)
        switch pixelFormat {
        case kCVPixelFormatType_DepthFloat32:
            let bytesPerPixel = MemoryLayout<Float32>.size
            let expectedBytesPerRow = width * bytesPerPixel
            
            // ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê²€ì‚¬
            guard bytesPerRow >= expectedBytesPerRow else {
                print("âŒ ARViewModel: Invalid bytesPerRow for Float32 depth")
                return nil
            }
            
            let rowData = baseAddress.advanced(by: y * bytesPerRow)
            let pixelData = rowData.assumingMemoryBound(to: Float32.self)
            let depthValue = pixelData[x]
            
            // ìœ íš¨í•œ depth ê°’ì¸ì§€ í™•ì¸ (ë” ì—„ê²©í•œ ì¡°ê±´)
            guard !depthValue.isNaN && !depthValue.isInfinite && depthValue > 0.1 && depthValue < 10.0 else {
                return nil
            }
            return depthValue
            
        case kCVPixelFormatType_DepthFloat16:
            let bytesPerPixel = MemoryLayout<UInt16>.size // Float16ì€ UInt16ìœ¼ë¡œ ì €ì¥ë¨
            let expectedBytesPerRow = width * bytesPerPixel
            
            // ë©”ëª¨ë¦¬ ì•ˆì „ì„± ê²€ì‚¬
            guard bytesPerRow >= expectedBytesPerRow else {
                print("âŒ ARViewModel: Invalid bytesPerRow for Float16 depth")
                return nil
            }
            
            let rowData = baseAddress.advanced(by: y * bytesPerRow)
            let pixelData = rowData.assumingMemoryBound(to: UInt16.self)
            let rawValue = pixelData[x]
            
            // âœ… UInt16ì„ Float16ìœ¼ë¡œ ë³€í™˜ í›„ Floatìœ¼ë¡œ ë³€í™˜
            let float16Value = Float16(bitPattern: rawValue)
            let depthValue = Float(float16Value)
            
            // ìœ íš¨í•œ depth ê°’ì¸ì§€ í™•ì¸
            guard !depthValue.isNaN && !depthValue.isInfinite && depthValue > 0.1 && depthValue < 10.0 else {
                return nil
            }
            return depthValue
            
        default:
            print("âŒ ARViewModel: Unsupported depth pixel format: \(pixelFormat.toString())")
            return nil
        }
    }
}

// Helper extensions
extension SIMD4 where Scalar == Float {
    var xyz: SIMD3<Float> {
        return SIMD3<Float>(x, y, z)
    }
}

// âœ… UIImage rotation extension
extension UIImage {
    func rotated(by angle: CGFloat) -> UIImage? {
        let rotatedSize = CGRect(origin: .zero, size: size)
            .applying(CGAffineTransform(rotationAngle: angle))
            .integral.size
        
        UIGraphicsBeginImageContext(rotatedSize)
        defer { UIGraphicsEndImageContext() }
        
        guard let context = UIGraphicsGetCurrentContext() else { return nil }
        
        let origin = CGPoint(x: rotatedSize.width / 2, y: rotatedSize.height / 2)
        context.translateBy(x: origin.x, y: origin.y)
        context.rotate(by: angle)
        
        draw(in: CGRect(
            x: -origin.y,
            y: -origin.x,
            width: size.width,
            height: size.height
        ))
        
        return UIGraphicsGetImageFromCurrentImageContext()
    }
}
