import Foundation
import Combine
import AVFoundation
import UIKit

private let TEMP_API_KEY = "" // ì‚¬ìš©ì ì œê³µ í‚¤ ìœ ì§€

@MainActor
class GeminiLiveAPIClient: NSObject, ObservableObject, URLSessionWebSocketDelegate {
    private var webSocketTask: URLSessionWebSocketTask?
    private let apiKey: String
    private var session: URLSession!

    @Published var isConnected: Bool = false
    @Published var isRecording: Bool = false
    @Published var isVideoEnabled: Bool = false
    @Published var chatMessages: [ChatMessage] = []
    @Published var currentTextInput: String = "" // í…ìŠ¤íŠ¸ ì…ë ¥ìš©
    @Published var currentModelResponse: String = "" // ì¶”ê°€ë¨ (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ìš©)
    
    // MARK: - AI Speaking State Management
    @Published var isAISpeaking = false
    @Published var hasPendingGuidanceRequest = false
    private var lastAIResponseTime = Date()
    
    // MARK: - Audio Engine Properties
    private var audioEngine: AVAudioEngine!
    private var audioPlayerNode: AVAudioPlayerNode!
    private var audioInputFormatForEngine: AVAudioFormat! // ì…ë ¥ìš© í¬ë§· (í•˜ë“œì›¨ì–´ ë˜ëŠ” ì„¸ì…˜ ê¸°ë³¸ê°’ ë”°ë¦„)
    private var audioOutputFormatForPCM: AVAudioFormat! // ìš°ë¦¬ PCM ë°ì´í„°ì˜ ì‹¤ì œ í¬ë§· (24kHz, 16bit, mono)
    private let audioSampleRate: Double = 24000.0
    private var isAudioEngineSetup = false
    
    // MARK: - Audio Input Properties
    private var inputTapInstalled = false
    private let audioQueue = DispatchQueue(label: "audioInput.queue", qos: .userInitiated)
    private var recordingTimer: Timer?
    private let recordingChunkDuration: TimeInterval = 0.1 // 100ms chunks for real-time
    private var accumulatedAudioData = Data()
    
    // **ì¶”ê°€: ARViewModel ì°¸ì¡°**
    weak var arViewModel: ARViewModel?
    
    // âœ… ì¶”ê°€: AppState ì°¸ì¡° (Stage ì²´í¬ìš©)
    weak var appState: AppState?
    
    // MARK: - Video Processing Properties
    @Published var debugProcessedImage: UIImage? = nil
    private let videoFrameInterval: TimeInterval = 0.5
    private let ciContext = CIContext()
    
    // âœ… íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext
    let reusableCIContext = CIContext(options: [.useSoftwareRenderer: false])

    init(apiKey: String = TEMP_API_KEY) {
        self.apiKey = apiKey
        super.init()
        self.session = URLSession(configuration: .default, delegate: self, delegateQueue: OperationQueue())
        setupAudioSession()
        setupAudioEngine()
    }

    // MARK: - Audio Session and Engine Setup
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
                        try audioSession.setCategory(.playAndRecord, 
                                        mode: .voiceChat, 
                                        options: [.allowBluetooth, .allowBluetoothA2DP])
            
            try audioSession.setPreferredSampleRate(audioSampleRate)
            try audioSession.setPreferredIOBufferDuration(0.02)
            
            try audioSession.setActive(true)
            print("Audio session setup complete for recording and playback.")
        } catch {
            print("Error setting up audio session: \(error)")
        }
    }

    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        audioPlayerNode = AVAudioPlayerNode()


        audioOutputFormatForPCM = AVAudioFormat(commonFormat: .pcmFormatInt16, 
                                              sampleRate: audioSampleRate, 
                                              channels: 1, 
                                              interleaved: true)
        
        if audioOutputFormatForPCM == nil {
            print("Error: Could not create audioOutputFormatForPCM.")
            isAudioEngineSetup = false
            return
        }
        
        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)
        audioInputFormatForEngine = AVAudioFormat(commonFormat: .pcmFormatInt16,
                                                sampleRate: audioSampleRate,
                                                channels: 1,
                                                interleaved: true)
        
        if audioInputFormatForEngine == nil {
            print("Error: Could not create audioInputFormatForEngine.")
            isAudioEngineSetup = false
            return
        }
        
        // í”Œë ˆì´ì–´ ë…¸ë“œ ì—°ê²°
        audioEngine.attach(audioPlayerNode)
        let mainMixer = audioEngine.mainMixerNode
        audioEngine.connect(audioPlayerNode, to: mainMixer, format: nil)

        do {
            audioEngine.prepare()
            try audioEngine.start()
            print("Audio engine started successfully for recording and playback.")
            isAudioEngineSetup = true
        } catch {
            print("Error starting audio engine: \(error)")
            isAudioEngineSetup = false
        }
    }
    
    private var setupParameters: (modelName: String, systemPrompt: String, voiceName: String, languageCode: String)?

    func connect(
        modelName: String = "models/gemini-2.0-flash-live-001",
        // modelName: String = "models/gemini-2.5-flash-preview-native-audio-dialog",
        systemPrompt: String = """
        You are a safety assistant for visually impaired users. Scan the ENTIRE camera view - left, right, center, top, bottom.

        IMMEDIATE ALERTS for ANY visible hazards:
        - Start with "ì£¼ì˜" + exact location + specific hazard type (person, car, bicycle, stairs, wet floor, etc.)
        - Moving objects: specify what's approaching (person, dog, vehicle, bicycle, etc.)
        - Obstacles: name specific items (chair, table, pole, box, etc.) and their height
        - Surfaces: specify exact condition (stairs, hole, wet tile, uneven concrete, etc.)

        CRITICAL: Don't only focus on center - scan FULL camera view width. Side objects are equally dangerous.

        RESPOND IN KOREAN. Always name specific objects/hazards, never use generic terms like "ë¬¼ì²´" or "ì¥ì• ë¬¼". ë¹ ë¥¸ ì†ë„ë¡œ ë§í•´ì£¼ì„¸ìš”.
        Examples: "ì£¼ì˜: ì™¼ìª½ì—ì„œ ìì „ê±° íƒ€ëŠ” ì‚¬ëŒì´ ì ‘ê·¼", "ì£¼ì˜: ì˜¤ë¥¸ìª½ì— ë‚®ì€ ë‚˜ë¬´ ì˜ì", "ì•ìª½ì— ìœ ë¦¬ë¬¸ì´ ìˆê³  ê¸¸ì´ ì•ˆì „í•´ìš”"

        Priority: Safety warnings > Navigation help > General chat
        """,
        voiceName: String = "Leda",
        languageCode: String = "ko-KR"
    ) {
        guard !apiKey.isEmpty, apiKey != "YOUR_API_KEY_HERE" else {
            let errorMessage = "Error: API Key is not set"
            print(errorMessage)
            self.chatMessages.append(ChatMessage(text: errorMessage, sender: .system))
            return
        }
        
        guard let url = URL(string: "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key=\(self.apiKey)") else {
        // guard let url = URL(string: "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent?key=\(self.apiKey)") else {
            print("Error: Invalid URL")
            self.chatMessages.append(ChatMessage(text: "Error: Invalid API URL", sender: .system))
            return
        }

        disconnect()
        
        webSocketTask = session.webSocketTask(with: url)
        webSocketTask?.resume()
        
        receiveMessagesLoop()
        
        // ì—°ê²° ì„±ê³µ í›„ setup ë©”ì‹œì§€ ì „ì†¡ì„ ìœ„í•´ ì €ì¥
        setupParameters = (modelName, systemPrompt, voiceName, languageCode)
    }
    
    func disconnect() {
        // ë…¹ìŒ ì¤‘ì´ë©´ ì¤‘ì§€
        if isRecording {
            stopRecording()
        }
        
        // âœ… ê°•í™”: ëª¨ë“  ì˜¤ë””ì˜¤ í™œë™ ì¤‘ë‹¨
        stopCurrentAudioPlayback()
        resetAISpeakingState()
        
        // âœ… ìºì‹œ ì œê±°: ë¹„ë””ì˜¤ ê´€ë ¨ ìƒíƒœ ë¦¬ì…‹ ì½”ë“œ ê°„ì†Œí™”
        DispatchQueue.main.async {
            self.isVideoEnabled = false
            self.debugProcessedImage = nil
        }
        
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
    }

    private func sendSetupMessage(
        modelName: String,
        systemPrompt: String,
        voiceName: String,
        languageCode: String
    ) {
        var currentModelName = modelName

        // ê³µì‹ ë¬¸ì„œì— ë”°ë¥¸ ì˜¬ë°”ë¥¸ ì–¸ì–´ ì½”ë“œ ë° ìŒì„± ì„¤ì •
        let prebuiltVoiceConfig = PrebuiltVoiceConfig(voiceName: voiceName)
        let voiceConfig = VoiceConfig(prebuiltVoiceConfig: prebuiltVoiceConfig)
        let speechConfig = SpeechConfig(
            languageCode: languageCode,
            voiceConfig: voiceConfig
        )
        let generationConfig = GenerationConfig(
            responseModalities: ["AUDIO"],
            speechConfig: speechConfig
        )
        
        // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        let systemInstruction = SystemInstruction(text: systemPrompt)        

        // Google Search Tool ì¶”ê°€
        let googleSearchTool = GoogleSearchTool()
        let tool = Tool(googleSearch: googleSearchTool)

        let config = GeminiLiveConfig(
            model: currentModelName,
            generationConfig: generationConfig,
            systemInstruction: systemInstruction,
            tools: [tool]
        )
        let setupMessage = SetupMessage(setup: config)
        
        do {
            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            let jsonData = try encoder.encode(setupMessage)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
                print("Sent SetupMessage with language \(languageCode) and voice \(voiceName): \(jsonString)")
            } else {
                print("Error: Could not convert SetupMessage to JSON string post-connection")
            }
        } catch {
            print("Error encoding SetupMessage post-connection: \(error)")
        }
    }

    func sendUserText(_ text: String) {
        guard isConnected, !text.isEmpty else { 
            print("Cannot send text: Not connected or text is empty.")
            return
        }
        
        var parts: [ClientTextPart] = [ClientTextPart(text: text)]
        
        // ë¹„ë””ì˜¤ê°€ í™œì„±í™”ë˜ì–´ ìˆë‹¤ë©´ í˜„ì¬ í”„ë ˆì„ì„ í•¨ê»˜ ì „ì†¡
        if isVideoEnabled, let currentVideoFrame = getCurrentVideoFrame() {
            parts.append(ClientTextPart(inlineData: InlineData(mimeType: "image/jpeg", data: currentVideoFrame)))
            
        } else if isVideoEnabled {
            // ë¹„ë””ì˜¤ í™œì„±í™”ë˜ì–´ ìˆì§€ë§Œ í”„ë ˆì„ ì—†ìŒ
        } else {
            // ë¹„ë””ì˜¤ ë¹„í™œì„±í™”ë¨
        }
        
        let turn = ClientTurn(role: "user", parts: parts)
        let clientTextPayload = ClientTextPayload(turns: [turn], turnComplete: true)
        let messageToSend = UserTextMessage(clientContent: clientTextPayload)
        
        do {
            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            let jsonData = try encoder.encode(messageToSend)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
            }
        } catch {
            print("Error encoding message: \(error)")
        }
    }

    // í˜„ì¬ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ìº¡ì²˜í•˜ëŠ” ë©”ì„œë“œ ìˆ˜ì •
    func getCurrentVideoFrame() -> String? {
        // âœ… í•­ìƒ ARViewModelì—ì„œ ì‹¤ì‹œê°„ ìµœì‹  í”„ë ˆì„ ìš”ì²­
        guard let arViewModel = arViewModel else {
            return nil
        }
        
        // âœ… í”„ë ˆì„ ìš”ì²­ ë¡œê¹…
        print("ğŸ”„ GeminiClient: Requesting fresh frame for text message")
        
        if let frame = arViewModel.getCurrentVideoFrameForGemini() {
            print("âœ… GeminiClient: Got fresh frame (\(frame.count) chars)")
            return frame
        } else {
            print("âŒ GeminiClient: No frame available")
            return nil
        }
    }

    private func sendString(_ string: String) {
        guard let task = webSocketTask else { 
            print("WebSocket task not available for sending string.")
            return
        }
        task.send(.string(string)) { error in
            if let error = error {
                print("Error sending string: \(error)")
            }
        }
    }
    
    private func sendData(_ data: Data) {
        guard let task = webSocketTask else { 
            print("WebSocket task not available for sending data.")
            return
        }
        task.send(.data(data)) { error in
            if let error = error {
                print("Error sending data: \(error)")
            }
        }
    }

    private func receiveMessagesLoop() {
        webSocketTask?.receive { [weak self] result in
            guard let self = self else { return }
            switch result {
            case .failure(let error):
                print("Error in receiving message: \(error)")
                // isConnectedëŠ” didCloseWithì—ì„œ ì²˜ë¦¬
                DispatchQueue.main.async {
                    self.chatMessages.append(ChatMessage(text: "Error receiving message: \(error.localizedDescription)", sender: .system))
                }
            case .success(let message):
                switch message {
                case .string(let text):
                    // âœ… ê°„ì†Œí™”ëœ ë¡œê¹… - ê¸´ ë°ì´í„° ë‚´ìš© ì œì™¸
                    if text.contains("\"inlineData\"") && text.contains("\"data\"") {
                        print("ğŸ“¥ Received large audio data response")
                    } else {
                        print("ğŸ“¥ Received text response")
                    }
                    self.parseServerMessage(text)
                    
                case .data(let data):
                    print("ğŸ“¥ Received \(data.count) bytes of data")
                    if let text = String(data: data, encoding: .utf8) {
                        self.parseServerMessage(text)
                    } else {
                        print("âŒ Could not convert data to string")
                    }
                @unknown default:
                    print("âŒ Unknown message type")
                }
                // ì—°ê²°ì´ í™œì„± ìƒíƒœì¼ ë•Œë§Œ ë‹¤ìŒ ë©”ì‹œì§€ë¥¼ ê³„ì† ìˆ˜ì‹ 
                if self.webSocketTask?.closeCode == .invalid { // closeCodeê°€ invalidë©´ ì•„ì§ í™œì„± ìƒíƒœë¡œ ê°„ì£¼
                    self.receiveMessagesLoop()
                }
            }
        }
    }
    
    private func parseServerMessage(_ jsonString: String) {
        guard let jsonData = jsonString.data(using: .utf8) else {
            print("âŒ Error: Could not convert JSON string to Data")
            return
        }
        
        let decoder = JSONDecoder()
        decoder.keyDecodingStrategy = .convertFromSnakeCase

        do {
            let wrapper = try decoder.decode(ServerResponseWrapper.self, from: jsonData)
            // âœ… ê°„ì†Œí™”ëœ ë¡œê¹… - ë°ì´í„° ë‚´ìš© ì œì™¸
            if wrapper.serverContent?.modelTurn?.parts.contains(where: { $0.inlineData != nil }) == true {
            } else if wrapper.serverContent?.modelTurn != nil {
                print("ğŸ’¬ Received text response")
            }

            var systemMessagesToAppend: [ChatMessage] = []
            var modelResponseText: String? = nil

            // 1. SetupComplete ì²˜ë¦¬
            if wrapper.setupComplete != nil {
                systemMessagesToAppend.append(ChatMessage(text: "System: Setup Complete! Ready to chat.", sender: .system))
            }

            // 2. ServerContentData ì²˜ë¦¬ (ëª¨ë¸ í…ìŠ¤íŠ¸/ì˜¤ë””ì˜¤, í„´ ìƒíƒœ ë“±)
            if let serverContent = wrapper.serverContent {
                
                // interrupted ìƒíƒœ ì²˜ë¦¬ - AI ì‘ë‹µ ì¤‘ë‹¨
                if let interrupted = serverContent.interrupted, interrupted {
                    stopAudioPlayback()
                    systemMessagesToAppend.append(ChatMessage(text: "System: Model response interrupted.", sender: .system))
                    handleAIResponseComplete(reason: "interrupted by server")
                }
                
                if let modelTurn = serverContent.modelTurn {
                    for part in modelTurn.parts {
                        if let text = part.text {
                            modelResponseText = (modelResponseText ?? "") + text
                        }
                        if let inlineData = part.inlineData {
                            // ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ í˜¸ì¶œ
                            handleReceivedAudioData(base64String: inlineData.data, mimeType: inlineData.mimeType)
                            handleAIResponseStart()
                        }
                        // ExecutableCode ì²˜ë¦¬
                        if let execCode = part.executableCode {
                            let lang = execCode.language ?? "Unknown language"
                            let code = execCode.code ?? "No code"
                            let execMessage = "Tool Execution Request:\nLanguage: \(lang)\nCode:\n\(code)"
                            systemMessagesToAppend.append(ChatMessage(text: execMessage, sender: .system, isToolResponse: true))
                        }
                    }
                }
                
                if let endOfTurn = serverContent.endOfTurn, endOfTurn {
                    systemMessagesToAppend.append(ChatMessage(text: "System: Model stream turn ended (endOfTurn=true).", sender: .system))
                    handleAIResponseComplete(reason: "endOfTurn received")
                }
                
                if let turnComplete = serverContent.turnComplete, turnComplete {
                    systemMessagesToAppend.append(ChatMessage(text: "System: Model reported turn complete (turnComplete=true).", sender: .system))
                    handleAIResponseComplete(reason: "turnComplete received")
                }
                
                if let generationComplete = serverContent.generationComplete, generationComplete {
                    systemMessagesToAppend.append(ChatMessage(text: "System: Model generation complete (generationComplete=true).", sender: .system))
                    handleAIResponseComplete(reason: "generationComplete received")
                }
            }

            // 3. ToolCall ì²˜ë¦¬ (FunctionCall from server)
            if let toolCall = wrapper.toolCall, let functionCalls = toolCall.functionCalls {
                for functionCall in functionCalls {
                    var toolMessageText = ""
                    if functionCall.name == "googleSearch" {
                        if let args = functionCall.args {
                            let resultText = args.map { "\($0.key): \($0.value)" }.joined(separator: "\n")
                            toolMessageText = "Google Search Result:\n---\n\(resultText)\n---"
                        } else {
                            toolMessageText = "Google Search called, but no arguments received."
                        }
                        systemMessagesToAppend.append(ChatMessage(text: toolMessageText, sender: .system, isToolResponse: true))
                        
                        if let callId = functionCall.id {
                            sendToolResponseMessage(id: callId, result: [:]) 
                        }
                    } else {
                        toolMessageText = "Received unhandled tool call: \(functionCall.name ?? "unknown")"
                        systemMessagesToAppend.append(ChatMessage(text: toolMessageText, sender: .system, isToolResponse: true))
                    }
                }
            }

            // 4. UsageMetadata ì²˜ë¦¬
            if let usage = wrapper.usageMetadata {
                var usageText = "Usage - Total Tokens: \(usage.totalTokenCount ?? 0)"
                if let promptTokens = usage.promptTokenCount, let responseTokens = usage.responseTokenCount {
                    usageText += " (Prompt: \(promptTokens), Response: \(responseTokens))"
                }
                systemMessagesToAppend.append(ChatMessage(text: "System: " + usageText, sender: .system))
            }

            // UI ì—…ë°ì´íŠ¸ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ)
            DispatchQueue.main.async {
                if let text = modelResponseText, !text.isEmpty {
                    self.chatMessages.append(ChatMessage(text: text, sender: .model))
                }
                self.chatMessages.append(contentsOf: systemMessagesToAppend)
            }

        } catch {
            print("âŒ Error decoding server message: \(error)")
        }
    }

    // MARK: - Tool Response Sender (NEW)
    func sendToolResponseMessage(id: String, result: [String: AnyCodableValue]) { // AnyCodableValueëŠ” ëª¨ë¸ íŒŒì¼ì— ì •ì˜ í•„ìš”
        guard isConnected else {
            print("Cannot send tool response: Not connected.")
            return
        }
        
        let functionResponse = FunctionResponse(id: id, response: result)
        let toolResponsePayload = ToolResponsePayload(functionResponses: [functionResponse])
        let messageToSend = ToolResponseMessage(toolResponse: toolResponsePayload)
        
        do {
            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            let jsonData = try encoder.encode(messageToSend)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
                print("Sent ToolResponseMessage: \(jsonString)")
            }
        } catch {
            print("Error encoding ToolResponseMessage: \(error)")
        }
    }

    // MARK: - Audio Handling Methods
    private func handleReceivedAudioData(base64String: String, mimeType: String) {
        // âœ… Stage 3ì—ì„œëŠ” ì£¼ê¸°ì  ê°€ì´ë˜ìŠ¤ë§Œ ì°¨ë‹¨í•˜ê³ , ì‚¬ìš©ì ì§ˆë¬¸ ì‘ë‹µì€ í—ˆìš©
        // (ì£¼ê¸°ì  ê°€ì´ë˜ìŠ¤ëŠ” AppStateì—ì„œ ê´€ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ëª¨ë“  ì˜¤ë””ì˜¤ í—ˆìš©)
        
        guard isAudioEngineSetup else {
            print("Audio engine not setup. Cannot play audio.")
            return
        }
        
        guard let audioData = Data(base64Encoded: base64String) else {
            print("Error: Could not decode base64 audio data.")
            return
        }
        
        // 1. ìš°ë¦¬ PCM ë°ì´í„°ì˜ ì‹¤ì œ í¬ë§· ì •ì˜ (audioOutputFormatForPCMì€ ì´ë¯¸ ë©¤ë²„ ë³€ìˆ˜ë¡œ ì¡´ì¬ ë° ì´ˆê¸°í™”ë¨)
        guard let sourceFormat = audioOutputFormatForPCM else {
            print("Error: audioOutputFormatForPCM (sourceFormat) is nil.")
            return
        }

        // 2. ì›ë³¸ ëª¨ë…¸ PCM ë²„í¼ ìƒì„±
        let monoBytesPerFrame = Int(sourceFormat.streamDescription.pointee.mBytesPerFrame)
        if monoBytesPerFrame == 0 {
            print("Error: monoBytesPerFrame is zero.")
            return
        }
        let monoFrameCount = AVAudioFrameCount(audioData.count / monoBytesPerFrame)
        if monoFrameCount == 0 {
            print("Error: Calculated monoFrameCount is zero.")
            // ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë„ˆë¬´ ì‘ê±°ë‚˜ í¬ë§· ë¬¸ì œì¼ ìˆ˜ ìˆìŒ. ì±„íŒ…ì— ë©”ì‹œì§€ ì¶”ê°€ ê°€ëŠ¥
            DispatchQueue.main.async {
                 self.chatMessages.append(ChatMessage(text: "System: Received audio data too small or invalid.", sender: .system))
            }
            return
        }
        
        guard let monoPCMBuffer = AVAudioPCMBuffer(pcmFormat: sourceFormat, frameCapacity: monoFrameCount) else {
            print("Error: Could not create monoPCMBuffer.")
            return
        }
        monoPCMBuffer.frameLength = monoFrameCount
        
        var dataCopiedSuccessfully = false
        audioData.withUnsafeBytes { (rawBufferPointer: UnsafeRawBufferPointer) in
            if let int16ChannelData = monoPCMBuffer.int16ChannelData, let sourceAddress = rawBufferPointer.baseAddress {
                // audioDataì˜ ë‚´ìš©ì„ monoPCMBufferì˜ int16 ì±„ë„ ë°ì´í„°ë¡œ ë³µì‚¬
                // UnsafeMutableRawBufferPointerì˜ countëŠ” ë³µì‚¬í•  ë°”ì´íŠ¸ ìˆ˜ (audioData.count)
                let destinationPointer = UnsafeMutableRawBufferPointer(start: int16ChannelData[0], count: audioData.count)
                memcpy(destinationPointer.baseAddress!, sourceAddress, audioData.count)
                dataCopiedSuccessfully = true
            } else {
                print("Error: monoPCMBuffer.int16ChannelData is nil or rawBufferPointer.baseAddress is nil.")
            }
        }
        
        guard dataCopiedSuccessfully else {
            print("Error: Failed to copy audio data to monoPCMBuffer.")
            return
        }

        // 3. íƒ€ê²Ÿ í¬ë§· ê°€ì ¸ì˜¤ê¸° (í”Œë ˆì´ì–´ ë…¸ë“œì˜ ì¶œë ¥ í¬ë§·)
        let targetFormat = audioPlayerNode.outputFormat(forBus: 0)

        // 4. í¬ë§· ë³€í™˜ ë° ì¬ìƒ
        if sourceFormat.isEqual(targetFormat) {
            // í¬ë§·ì´ ë™ì¼í•˜ë©´ ì§ì ‘ ìŠ¤ì¼€ì¤„ (ë§¤ìš° ë“œë¬¸ ê²½ìš°)
            // print("Source and target audio formats are the same. Scheduling directly.")
            audioPlayerNode.scheduleBuffer(monoPCMBuffer) { /* completion */ }
        } else {
            // í¬ë§·ì´ ë‹¤ë¥´ë©´ ë³€í™˜ í•„ìš”
            // print("Source format: \(sourceFormat), Target format: \(targetFormat). Conversion needed.")
            guard let converter = AVAudioConverter(from: sourceFormat, to: targetFormat) else {
                print("Error: Could not create AVAudioConverter from \(sourceFormat) to \(targetFormat)")
                return
            }

            // ë³€í™˜ëœ ë²„í¼ì˜ ì˜ˆìƒ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            // frameLengthê°€ ì•„ë‹Œ frameCapacityë¥¼ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ë„ ìˆìŒ. monoPCMBuffer.frameLengthëŠ” ì´ë¯¸ ì„¤ì •ë¨.
            let outputFrameCapacity = AVAudioFrameCount(ceil(Double(monoPCMBuffer.frameLength) * (targetFormat.sampleRate / sourceFormat.sampleRate)))
            guard outputFrameCapacity > 0 else {
                print("Error: outputFrameCapacity is zero or negative (\(outputFrameCapacity)). Input frames: \(monoPCMBuffer.frameLength), SR Ratio: \(targetFormat.sampleRate / sourceFormat.sampleRate)")
                return
            }

            guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
                print("Error: Could not create convertedBuffer for targetFormat. Capacity: \(outputFrameCapacity)")
                return
            }

            var error: NSError?
            // inputBufferProvidedëŠ” ê° convert í˜¸ì¶œì— ëŒ€í•´ ë¡œì»¬ ìƒíƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
            // ì´ í´ë¡œì €ëŠ” ì¦‰ì‹œ ì‹¤í–‰ë˜ë¯€ë¡œ, handleReceivedAudioData í˜¸ì¶œë§ˆë‹¤ ìƒˆë¡œ ìƒì„±ë©ë‹ˆë‹¤.
            var inputBufferProvidedForThisConversion = false 

            // ì…ë ¥ ë¸”ë¡: ë³€í™˜ê¸°ì— ì›ë³¸ ë°ì´í„°ë¥¼ ì œê³µ
            let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
                if inputBufferProvidedForThisConversion {
                    outStatus.pointee = .endOfStream // ì´ë¯¸ monoPCMBufferë¥¼ ì œê³µí–ˆìœ¼ë¯€ë¡œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
                    return nil
                }
                // monoPCMBufferë¥¼ ì œê³µí•˜ê³ , í•œ ë²ˆ ì œê³µí–ˆìŒì„ í‘œì‹œ
                outStatus.pointee = .haveData
                inputBufferProvidedForThisConversion = true
                return monoPCMBuffer
            }
            
            // ë³€í™˜ ì‹¤í–‰
            let status = converter.convert(to: convertedBuffer, error: &error, withInputFrom: inputBlock)

            if status == .error {
                print("Error during audio conversion: \(error?.localizedDescription ?? "Unknown error")")
                if let nsError = error {
                     DispatchQueue.main.async {
                         self.chatMessages.append(ChatMessage(text: "System: Audio conversion error - \(nsError.code)", sender: .system))
                    }
                }
                return
            }
            
            // ë³€í™˜ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìŠ¤ì¼€ì¤„
            if convertedBuffer.frameLength > 0 {
                // print("Scheduling converted buffer with \(convertedBuffer.frameLength) frames.")
                audioPlayerNode.scheduleBuffer(convertedBuffer) { /* completion */ }
            } else if status != .error { // ì—ëŸ¬ëŠ” ì•„ë‹ˆì§€ë§Œ ë³€í™˜ëœ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
                print("Audio conversion resulted in an empty buffer (length: \(convertedBuffer.frameLength)). Status: \(status.rawValue)")
                 DispatchQueue.main.async {
                     self.chatMessages.append(ChatMessage(text: "System: Audio conversion yielded no data (status \(status.rawValue)).", sender: .system))
                 }
            }
        }
        
        // í”Œë ˆì´ì–´ ì‹œì‘ (í•„ìš”í•œ ê²½ìš°)
        if !audioPlayerNode.isPlaying {
            // print("Audio player node is not playing. Starting player.")
            audioPlayerNode.play()
        }
    }
    
    // MARK: - Audio Playback Control
    
    private func stopAudioPlayback() {
        guard isAudioEngineSetup else { return }
        
        // í˜„ì¬ ì¬ìƒì¤‘ì¸ ì˜¤ë””ì˜¤ ì¦‰ì‹œ ì¤‘ë‹¨
        audioPlayerNode.stop()
        
        // ìŠ¤ì¼€ì¤„ëœ ë²„í¼ë“¤ ëª¨ë‘ ì œê±°
        audioPlayerNode.reset()
        
        // âœ… AI speaking ìƒíƒœ ì¦‰ì‹œ ì¤‘ë‹¨
        DispatchQueue.main.async {
            self.isAISpeaking = false
            self.hasPendingGuidanceRequest = false
            self.lastAIResponseTime = Date()
            print("ğŸ”‡ GeminiClient: AI speaking interrupted")
        }
        
        print("Audio playback stopped and buffers cleared")
    }
    
    // MARK: - AI Speaking State Management
    private func handleAIResponseStart() {
        DispatchQueue.main.async {
            if !self.isAISpeaking {
                self.isAISpeaking = true
                self.lastAIResponseTime = Date()
                print("ğŸ”Š GeminiClient: AI started speaking (audio content received)")
            }
        }
    }
    
    private func handleAIResponseComplete(reason: String) {
        DispatchQueue.main.async {
            if self.isAISpeaking {
                self.isAISpeaking = false
                self.hasPendingGuidanceRequest = false
                self.lastAIResponseTime = Date()
                print("ğŸ”‡ GeminiClient: AI finished speaking (\(reason))")
            }
        }
    }
    
    func canSendGuidanceRequest() -> Bool {
        return !isAISpeaking && !hasPendingGuidanceRequest
    }
    
    // MARK: - Audio Recording Methods
    
    // OLD version - to be replaced
    // func requestMicrophonePermission(completion: @escaping (Bool) -> Void) {
    //     AVAudioSession.sharedInstance().requestRecordPermission { granted in
    //         DispatchQueue.main.async {
    //             completion(granted)
    //         }
    //     }
    // }

    // NEW async version
    func requestMicrophonePermission() async -> Bool {
        await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
    }
    
    // Made public so AppState can control audio recording during STT
    func startRecording() { 
        guard isConnected else {
            print("GeminiLiveAPIClient: Cannot start recording - not connected to server")
            DispatchQueue.main.async {
                self.chatMessages.append(ChatMessage(text: "System: Cannot start recording - not connected", sender: .system))
            }
            return
        }
        
        guard isAudioEngineSetup else {
            print("GeminiLiveAPIClient: Cannot start recording - audio engine not setup")
            return
        }
        
        // **ê°œì„ : ì˜¤ë””ì˜¤ ì„¸ì…˜ ìƒíƒœ í™•ì¸**
        let audioSession = AVAudioSession.sharedInstance()
        print("GeminiLiveAPIClient: Audio session category: \(audioSession.category), active: \(audioSession.isOtherAudioPlaying)")
        
        print("GeminiLiveAPIClient: Starting recording (called externally)")
        
        // **ìˆ˜ì •: ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë¡œì§ ë³µì›**
        Task {
            // Microphone permission and start
            if await requestMicrophonePermission() { 
                await MainActor.run { 
                    self.startRecordingInternal()
                }
            } else {
                await MainActor.run {
                    self.chatMessages.append(ChatMessage(text: "System: Microphone permission denied for manual start.", sender: .system))
                }
                print("GeminiLiveAPIClient: Microphone permission denied during manual start.")
            }
        }
    }
    
    private func startRecordingInternal() {
        guard !isRecording else { 
            print("GeminiLiveAPIClient: Already recording, skipping")
            return 
        }
        
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            
            // ì…ë ¥ íƒ­ ì„¤ì¹˜
            if self.installInputTap() {
                self.startRecordingTimer()
                DispatchQueue.main.async {
                    self.isRecording = true
                    self.chatMessages.append(ChatMessage(text: "System: Recording started", sender: .system))
                }
                print("GeminiLiveAPIClient: Recording started successfully")
            } else {
                print("GeminiLiveAPIClient: Failed to install input tap")
                DispatchQueue.main.async {
                    self.chatMessages.append(ChatMessage(text: "System: Failed to start recording", sender: .system))
                }
            }
        }
    }
    
    func stopRecording() {
        print("GeminiLiveAPIClient: Stopping recording (called externally)")
        
        // **ê°œì„ : ì˜¤ë””ì˜¤ ì„¸ì…˜ ìƒíƒœ ë¡œê¹…**
        let audioSession = AVAudioSession.sharedInstance()
        print("GeminiLiveAPIClient: Before stop - Audio session category: \(audioSession.category), active: \(audioSession.isOtherAudioPlaying)")
        
        guard isRecording else { 
            print("GeminiLiveAPIClient: Not recording, skipping stop")
            return 
        }
        
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            
            // íƒ€ì´ë¨¸ ì •ì§€
            DispatchQueue.main.async {
                self.recordingTimer?.invalidate()
                self.recordingTimer = nil
            }
            
            // ì…ë ¥ íƒ­ ì œê±°
            if self.inputTapInstalled {
                self.audioEngine.inputNode.removeTap(onBus: 0)
                self.inputTapInstalled = false
            }
            
            // ë§ˆì§€ë§‰ ëˆ„ì ëœ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì†¡
            if !self.accumulatedAudioData.isEmpty {
                self.sendAccumulatedAudioData()
            }
            
            DispatchQueue.main.async {
                self.isRecording = false
                self.chatMessages.append(ChatMessage(text: "System: Recording stopped", sender: .system))
            }
            print("GeminiLiveAPIClient: Recording stopped")
        }
    }
    
    private func installInputTap() -> Bool {
        guard !inputTapInstalled else { return true }
        
        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)
        
        print("Installing input tap with format: \(inputFormat)")
        
        // ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒ­ ì„¤ì¹˜
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, time in
            self?.processAudioBuffer(buffer)
        }
        
        inputTapInstalled = true
        return true
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let targetFormat = audioInputFormatForEngine else { return }
        
        // í¬ë§· ë³€í™˜ì´ í•„ìš”í•œì§€ í™•ì¸
        let sourceFormat = buffer.format
        
        if sourceFormat.isEqual(targetFormat) {
            // í¬ë§·ì´ ë™ì¼í•˜ë©´ ì§ì ‘ ì‚¬ìš©
            saveAudioDataFromBuffer(buffer)
        } else {
            // í¬ë§· ë³€í™˜ í•„ìš”
            convertAndSaveAudioBuffer(buffer, to: targetFormat)
        }
    }
    
    private func convertAndSaveAudioBuffer(_ sourceBuffer: AVAudioPCMBuffer, to targetFormat: AVAudioFormat) {
        guard let converter = AVAudioConverter(from: sourceBuffer.format, to: targetFormat) else {
            print("Error: Could not create audio converter for recording")
            return
        }
        
        // ë³€í™˜ëœ ë²„í¼ í¬ê¸° ê³„ì‚°
        let outputFrameCapacity = AVAudioFrameCount(ceil(Double(sourceBuffer.frameLength) * 
                                                        (targetFormat.sampleRate / sourceBuffer.format.sampleRate)))
        
        guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
            print("Error: Could not create converted buffer for recording")
            return
        }
        
        var error: NSError?
        var inputBufferProvided = false
        
        let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
            if inputBufferProvided {
                outStatus.pointee = .endOfStream
                return nil
            }
            outStatus.pointee = .haveData
            inputBufferProvided = true
            return sourceBuffer
        }
        
        let status = converter.convert(to: convertedBuffer, error: &error, withInputFrom: inputBlock)
        
        if status == .error {
            print("Error during audio conversion for recording: \(error?.localizedDescription ?? "Unknown error")")
            return
        }
        
        if convertedBuffer.frameLength > 0 {
            saveAudioDataFromBuffer(convertedBuffer)
        }
    }
    
    private func saveAudioDataFromBuffer(_ buffer: AVAudioPCMBuffer) {
        let frameLength = Int(buffer.frameLength)
        let channelCount = Int(buffer.format.channelCount)
        
        guard frameLength > 0 else {
            return
        }
        
        var audioData: Data?
        
        if buffer.format.commonFormat == .pcmFormatInt16 {
            guard let int16ChannelData = buffer.int16ChannelData else { 
                return 
            }
            let dataSize = frameLength * channelCount * MemoryLayout<Int16>.size
            audioData = Data(bytes: int16ChannelData[0], count: dataSize)
            
        } else if buffer.format.commonFormat == .pcmFormatFloat32 {
            guard let floatChannelData = buffer.floatChannelData else { 
                return 
            }
            
            // Float32ë¥¼ Int16ìœ¼ë¡œ ë³€í™˜
            var int16Array = Array<Int16>(repeating: 0, count: frameLength * channelCount)
            for frame in 0..<frameLength {
                for channel in 0..<channelCount {
                    let floatValue = floatChannelData[channel][frame]
                    let clampedValue = max(-1.0, min(1.0, floatValue))
                    int16Array[frame * channelCount + channel] = Int16(clampedValue * 32767.0)
                }
            }
            
            audioData = Data(bytes: int16Array, count: int16Array.count * MemoryLayout<Int16>.size)
            
        } else {
            return
        }
        
        guard let validAudioData = audioData else {
            return
        }
        
        // ëˆ„ì  ë°ì´í„°ì— ì¶”ê°€
        accumulatedAudioData.append(validAudioData)
    }
    
    private func startRecordingTimer() {
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.recordingTimer = Timer.scheduledTimer(withTimeInterval: self.recordingChunkDuration, repeats: true) { _ in
                self.audioQueue.async {
                    self.sendAccumulatedAudioData()
                }
            }
        }
    }
    
    private func sendAccumulatedAudioData() {
        guard !accumulatedAudioData.isEmpty else { 
            return 
        }
        
        let dataSize = accumulatedAudioData.count
        
        // Base64ë¡œ ì¸ì½”ë”©
        let base64Data = accumulatedAudioData.base64EncodedString()
        
        // ì‹¤ì‹œê°„ ì…ë ¥ ë©”ì‹œì§€ ìƒì„± ë° ì „ì†¡
        sendRealtimeAudioInput(base64Data: base64Data)
        
        // ë°ì´í„° ì´ˆê¸°í™”
        accumulatedAudioData = Data()
    }
    
    private func sendRealtimeAudioInput(base64Data: String) {
        let mediaChunk = RealtimeMediaChunk(mimeType: "audio/pcm;rate=24000", data: base64Data)
        let realtimeInput = RealtimeInputPayload(mediaChunks: [mediaChunk])
        let message = RealtimeInputMessage(realtimeInput: realtimeInput)
        
        do {
            let encoder = JSONEncoder()
            let jsonData = try encoder.encode(message)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
            }
        } catch {
            print("âŒ GeminiLiveAPIClient: Audio encoding error: \(error)")
        }
    }

    // MARK: - URLSessionWebSocketDelegate

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        DispatchQueue.main.async {
            self.isConnected = true
            self.chatMessages.append(ChatMessage(text: "System: WebSocket Connected!", sender: .system))
        }
        print("WebSocket connection opened")
        
        // Send setup message with stored parameters
        if let params = setupParameters {
            sendSetupMessage(
                modelName: params.modelName,
                systemPrompt: params.systemPrompt,
                voiceName: params.voiceName,
                languageCode: params.languageCode
            )
        }
        
        // Start audio recording
        Task {
            if await requestMicrophonePermission() {
                await MainActor.run { self.startRecordingInternal() }
            } else {
                await MainActor.run {
                    self.chatMessages.append(ChatMessage(text: "System: Microphone permission denied for auto-start.", sender: .system))
                }
                print("Microphone permission denied during auto-start for GeminiLiveAPIClient.")
            }
        }
    }

    func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        DispatchQueue.main.async {
            if self.isConnected {
                var reasonString = "Unknown reason"
                if let reasonData = reason, let str = String(data: reasonData, encoding: .utf8), !str.isEmpty {
                    reasonString = str
                }
                self.chatMessages.append(ChatMessage(text: "System: WebSocket Disconnected. Code: \(closeCode.rawValue), Reason: \(reasonString)", sender: .system))
            }
            self.isConnected = false
        }
        var reasonStringLog = ""
        if let reason = reason, let str = String(data: reason, encoding: .utf8) {
            reasonStringLog = str
        }
        print("WebSocket connection closed: code \(closeCode.rawValue), reason: \(reasonStringLog)")
        self.webSocketTask = nil
    }
    
    // MARK: - Cleanup
    deinit {
        recordingTimer?.invalidate()
        recordingTimer = nil
        
        if inputTapInstalled {
            audioEngine.inputNode.removeTap(onBus: 0)
            inputTapInstalled = false
        }
        
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
        print("GeminiLiveAPIClient deinitialized")
    }

    // MARK: - Video Frame Processing
    
    // âœ… ìƒˆë¡œìš´ ì¦‰ì‹œ ë™ê¸°ì  í”„ë ˆì„ ì „ì†¡ ë©”ì„œë“œ (ë”œë ˆì´ ìµœì†Œí™”)
    func sendVideoFrameImmediately(pixelBuffer: CVPixelBuffer) {
        guard isConnected else {
            return
        }
        
        // âœ… ë¹„ë””ì˜¤ í™œì„±í™” (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
        if !isVideoEnabled {
            isVideoEnabled = true
        }
        
        // âœ… CVPixelBufferë¥¼ CIImageë¡œ ë³€í™˜ (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
        var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // âœ… iOS ì¹´ë©”ë¼ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê°€ë¡œ ë°©í–¥ì´ë¯€ë¡œ ì„¸ë¡œë¡œ íšŒì „
        ciImage = ciImage.oriented(.right)
        
        // âœ… 0.5ë°° ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ë°ì´í„° í¬ê¸° ì¤„ì„
        let targetScale: CGFloat = 0.5
        let scaleTransform = CGAffineTransform(scaleX: targetScale, y: targetScale)
        ciImage = ciImage.transformed(by: scaleTransform)
        
        // âœ… JPEG ë°ì´í„° ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì„±ëŠ¥ í–¥ìƒ)
        guard let jpegData = reusableCIContext.jpegRepresentation(
            of: ciImage,
            colorSpace: ciImage.colorSpace ?? CGColorSpaceCreateDeviceRGB(),
            options: [kCGImageDestinationLossyCompressionQuality as CIImageRepresentationOption: 0.8]
        ) else {
            print("âŒ GeminiLiveAPIClient: Failed to create JPEG data")
            return
        }
        
        // âœ… Base64 ì¸ì½”ë”©
        let base64ImageData = jpegData.base64EncodedString()
        
        // âœ… ë””ë²„ê·¸ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
        debugProcessedImage = UIImage(data: jpegData)
        
        // âœ… Geminiì— ì¦‰ì‹œ ì „ì†¡
        sendRealtimeVideoFrame(base64Data: base64ImageData)
        
        // âœ… ë¡œê¹… ê°„ì†Œí™” (10ì´ˆë§ˆë‹¤ í•œ ë²ˆë§Œ ë¡œê·¸)
        let shouldLog = Int(Date().timeIntervalSince1970) % 10 == 0
        if shouldLog {
            print("ğŸ“¹ GeminiLiveAPIClient: Video transmission active (\(base64ImageData.count) chars, \(jpegData.count) bytes)")
        }
    }
    
    // âœ… ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ í”„ë ˆì„ ì „ì†¡ ë©”ì„œë“œ (ë¡œê¹… ê°„ì†Œí™”)
    private func sendRealtimeVideoFrame(base64Data: String) {
        let mediaChunk = RealtimeMediaChunk(mimeType: "image/jpeg", data: base64Data)
        let realtimeInput = RealtimeInputPayload(mediaChunks: [mediaChunk])
        let message = RealtimeInputMessage(realtimeInput: realtimeInput)
        
        do {
            let encoder = JSONEncoder()
            let jsonData = try encoder.encode(message)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
                // âœ… ë¡œê¹… ê°„ì†Œí™” (ë§¤ì´ˆë§ˆë‹¤ ì¶œë ¥ë˜ëŠ” ê²ƒì„ ë°©ì§€)
                let shouldLog = Int(Date().timeIntervalSince1970) % 10 == 0
                if shouldLog {
                    print("ğŸ“¹ GeminiLiveAPIClient: Video frame transmission active (\(base64Data.count) chars)")
                }
            }
        } catch {
            print("âŒ GeminiLiveAPIClient: Video frame encoding error: \(error)")
        }
    }
    
    // âœ… ê¸°ì¡´ ë©”ì„œë“œëŠ” ë ˆê±°ì‹œìš©ìœ¼ë¡œ ìœ ì§€í•˜ë˜ ê°œì„ 
    func processAndSendVideoFrame(pixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation = .up, timestamp: TimeInterval) {
        // âœ… ìƒˆë¡œìš´ ì¦‰ì‹œ ì „ì†¡ ë©”ì„œë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        sendVideoFrameImmediately(pixelBuffer: pixelBuffer)
    }

    // MARK: - Object Matching via Gemini API
    func findSimilarObject(koreanObjectName: String, availableObjects: [String], completion: @escaping (String?) -> Void) {
        let requestId = UUID().uuidString.prefix(8) // âœ… ìš”ì²­ ì¶”ì ìš© ID
        print("ğŸ” GeminiLiveAPIClient: [REQ-\(requestId)] Starting object similarity request")
        print("   Korean object: '\(koreanObjectName)'")
        print("   Available objects (\(availableObjects.count)): \(availableObjects.joined(separator: ", "))")
        
        let prompt = """
        You are helping with object detection matching. 
        
        User requested object in Korean: "\(koreanObjectName)"
        Available detected objects in English: \(availableObjects.joined(separator: ", "))
        
        Find the most similar English object name from the available list that matches the Korean object name.
        Reply with ONLY the exact English object name from the list, or "NOT_FOUND" if no reasonable match exists.
        
        Examples:
        - Korean "ì˜ì" should match English "chair"
        - Korean "ì±…ìƒ" should match English "table" or "dining table"  
        - Korean "ì¹¨ëŒ€" should match English "bed"
        - Korean "ì†ŒíŒŒ" should match English "couch"
        - Korean "ì»´í“¨í„°" should match English "laptop"
        - Korean "ë…¸íŠ¸ë¶" should match English "laptop"
        - Korean "í•¸ë“œí°" should match English "cell phone"
        
        Reply format: [OBJECT_NAME] or NOT_FOUND
        """
        
        // Use REST API for quick object matching
        sendRESTRequest(prompt: prompt) { [weak self] response in
            DispatchQueue.main.async {
                let result = response?.trimmingCharacters(in: .whitespacesAndNewlines)
                let matchedObject = (result == "NOT_FOUND" || result?.isEmpty == true) ? nil : result
                
                print("âœ… GeminiLiveAPIClient: [REQ-\(requestId)] Object matching completed")
                print("   Raw response: '\(response ?? "nil")'")
                print("   Processed result: '\(matchedObject ?? "NOT_FOUND")'")
                print("   Korean '\(koreanObjectName)' â†’ English '\(matchedObject ?? "NOT_FOUND")'")
                
                completion(matchedObject)
            }
        }
    }
    
    private func sendRESTRequest(prompt: String, completion: @escaping (String?) -> Void) {
        // Simple REST API call to Gemini for object matching
        guard let url = URL(string: "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=\(apiKey)") else {
            print("GeminiLiveAPIClient: Invalid REST API URL")
            completion(nil)
            return
        }
        
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        let requestBody: [String: Any] = [
            "contents": [
                [
                    "parts": [
                        ["text": prompt]
                    ]
                ]
            ]
        ]
        
        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
        } catch {
            print("GeminiLiveAPIClient: Failed to serialize REST request: \(error)")
            completion(nil)
            return
        }
        
        URLSession.shared.dataTask(with: request) { data, response, error in
            if let error = error {
                print("GeminiLiveAPIClient: REST API error: \(error)")
                completion(nil)
                return
            }
            
            guard let data = data else {
                print("GeminiLiveAPIClient: No data received from REST API")
                completion(nil)
                return
            }
            
            do {
                if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any],
                   let candidates = json["candidates"] as? [[String: Any]],
                   let firstCandidate = candidates.first,
                   let content = firstCandidate["content"] as? [String: Any],
                   let parts = content["parts"] as? [[String: Any]],
                   let firstPart = parts.first,
                   let text = firstPart["text"] as? String {
                    completion(text)
                } else {
                    print("GeminiLiveAPIClient: Failed to parse REST API response")
                    completion(nil)
                }
            } catch {
                print("GeminiLiveAPIClient: Failed to parse REST API JSON: \(error)")
                completion(nil)
            }
        }.resume()
    }

    // âœ… ìƒˆë¡œìš´ ë©”ì„œë“œ: í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ì˜¤ë””ì˜¤ ì¦‰ì‹œ ì¤‘ë‹¨
    func stopCurrentAudioPlayback() {
        print("GeminiLiveAPIClient: Stopping current audio playback")
        
        // AudioPlayerNodeì˜ ëª¨ë“  ì˜¤ë””ì˜¤ ì¤‘ë‹¨
        if isAudioEngineSetup {
            audioPlayerNode.stop()
            audioPlayerNode.reset()
            print("GeminiLiveAPIClient: AudioPlayerNode stopped and reset")
        }
        
        // ì§„í–‰ ì¤‘ì¸ ëª¨ë“  ì˜¤ë””ì˜¤ ë²„í¼ ì œê±°
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            self.accumulatedAudioData = Data()
            print("GeminiLiveAPIClient: Cleared accumulated audio data")
        }
    }
    
    // âœ… ìƒˆë¡œìš´ ë©”ì„œë“œ: AI ìƒíƒœ í”Œë˜ê·¸ ë¦¬ì…‹
    func resetAISpeakingState() {
        DispatchQueue.main.async {
            self.isAISpeaking = false
            self.hasPendingGuidanceRequest = false
            self.lastAIResponseTime = Date()
            print("GeminiLiveAPIClient: AI speaking state reset")
        }
    }

    // **ì¶”ê°€: GeminiClientìš© ìµœì‹  í”„ë ˆì„ ì œê³µ ë©”ì„œë“œ**
    func getCurrentVideoFrameForGemini() -> String? {
        // âœ… ARViewModelì—ì„œ í”„ë ˆì„ì„ ê°€ì ¸ì™€ì•¼ í•¨ (URLSessionì´ ì•„ë‹Œ ARSession í•„ìš”)
        guard let arViewModel = arViewModel else {
            print("âŒ GeminiLiveAPIClient: ARViewModel not available")
            return nil
        }
        
        // âœ… ARSessionì˜ currentFrame ì‚¬ìš©
        guard let currentFrame = arViewModel.session.currentFrame else {
            print("âŒ GeminiLiveAPIClient: No current frame from ARSession")
            return nil
        }
        
        // âœ… ì¦‰ì‹œ í•„ìš”í•œ ë°ì´í„°ë§Œ ë³µì‚¬í•˜ê³  ARFrame ì°¸ì¡° í•´ì œ
        let pixelBuffer = currentFrame.capturedImage
        
        // âœ… autoreleasepoolë¡œ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ + ìì‹ ì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext ì‚¬ìš©
        return autoreleasepool {
            var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            ciImage = ciImage.oriented(.right)
            
            let targetScale: CGFloat = 0.5
            let scaleTransform = CGAffineTransform(scaleX: targetScale, y: targetScale)
            ciImage = ciImage.transformed(by: scaleTransform)
            
            // âœ… ìì‹ ì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext ì‚¬ìš© (self ì‚¬ìš©)
            guard let jpegData = self.reusableCIContext.jpegRepresentation(
                of: ciImage,
                colorSpace: ciImage.colorSpace ?? CGColorSpaceCreateDeviceRGB(),
                options: [kCGImageDestinationLossyCompressionQuality as CIImageRepresentationOption: 0.8]
            ) else {
                print("âŒ GeminiLiveAPIClient: Failed to create JPEG from current frame")
                return nil
            }
            
            return jpegData.base64EncodedString()
        }
    }
} 
